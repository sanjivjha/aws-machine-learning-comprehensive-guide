{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65814e74",
   "metadata": {},
   "source": [
    "# End-to-End IIoT Predictive Maintenance for Oil Pumps on AWS\n",
    "\n",
    "This case study demonstrates a complete workflow for predictive maintenance of oil pumps using AWS IoT and machine learning services. We'll collect sensor data, detect anomalies, and predict potential equipment failures.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- An AWS account with appropriate permissions\n",
    "- AWS CLI configured with your credentials\n",
    "- Python 3.7 or later\n",
    "- Required Python packages: boto3, pandas, numpy, matplotlib, scikit-learn\n",
    "\n",
    "Install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9536f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install boto3 pandas numpy matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9856100a",
   "metadata": {},
   "source": [
    "## Step 1: IoT Device Simulation and Data Ingestion\n",
    "\n",
    "First, we'll simulate IoT devices sending sensor data from oil pumps. In a real-world scenario, you would replace this with actual IoT devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b91b926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# Create an IoT client\n",
    "iot = boto3.client('iot-data')\n",
    "\n",
    "# Define the IoT topic\n",
    "topic = 'oil_pump/sensors'\n",
    "\n",
    "def generate_sensor_data(pump_id):\n",
    "    return {\n",
    "        'pump_id': pump_id,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'temperature': random.uniform(50, 100),\n",
    "        'pressure': random.uniform(100, 500),\n",
    "        'flow_rate': random.uniform(200, 1000),\n",
    "        'vibration': random.uniform(0.1, 5.0)\n",
    "    }\n",
    "\n",
    "# Simulate data from 5 pumps for 1 minute\n",
    "for _ in range(60):\n",
    "    for pump_id in range(1, 6):\n",
    "        sensor_data = generate_sensor_data(pump_id)\n",
    "        \n",
    "        # Publish to IoT Core\n",
    "        iot.publish(\n",
    "            topic=topic,\n",
    "            qos=1,\n",
    "            payload=json.dumps(sensor_data)\n",
    "        )\n",
    "    \n",
    "    time.sleep(1)  # Wait for 1 second\n",
    "\n",
    "print(\"Data simulation completed.\")\n",
    "\n",
    "# In a real-world scenario, replace this simulation with actual IoT device code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9170aa1d",
   "metadata": {},
   "source": [
    "## Step 2: Data Processing with AWS IoT Rules and AWS Lambda\n",
    "\n",
    "We'll use AWS IoT Rules to route the incoming data to a Lambda function for processing and storage.\n",
    "\n",
    "First, create an AWS IoT Rule:\n",
    "\n",
    "1. Go to the AWS IoT Core console\n",
    "2. Navigate to \"Act\" > \"Rules\" and click \"Create\"\n",
    "3. Set the rule name (e.g., \"oil_pump_data_processing\")\n",
    "4. Set the rule query statement:\n",
    "   ```sql\n",
    "   SELECT * FROM 'oil_pump/sensors'\n",
    "   ```\n",
    "5. Add an action to invoke a Lambda function (we'll create this function next)\n",
    "\n",
    "Now, let's create the Lambda function for data processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d6f654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize DynamoDB client\n",
    "dynamodb = boto3.resource('dynamodb')\n",
    "table = dynamodb.Table(os.environ['DYNAMODB_TABLE'])\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Parse the incoming IoT data\n",
    "    iot_data = json.loads(json.dumps(event))\n",
    "    \n",
    "    # Add a timestamp for when the data was processed\n",
    "    iot_data['processed_timestamp'] = datetime.now().isoformat()\n",
    "    \n",
    "    # Store the data in DynamoDB\n",
    "    table.put_item(Item=iot_data)\n",
    "    \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps('Data processed and stored successfully!')\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162df4bb",
   "metadata": {},
   "source": [
    "Deploy this Lambda function and set up the necessary permissions for it to write to DynamoDB. Also, create a DynamoDB table named \"oil_pump_sensor_data\" with \"pump_id\" as the partition key and \"timestamp\" as the sort key.\n",
    "\n",
    "## Step 3: Data Analysis and Anomaly Detection\n",
    "\n",
    "We'll use Amazon SageMaker to train an anomaly detection model using the collected sensor data.\n",
    "\n",
    "First, let's retrieve the data from DynamoDB and prepare it for modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ca4222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from boto3.dynamodb.conditions import Key\n",
    "\n",
    "dynamodb = boto3.resource('dynamodb')\n",
    "table = dynamodb.Table('oil_pump_sensor_data')\n",
    "\n",
    "# Retrieve data for the last 24 hours\n",
    "response = table.query(\n",
    "    KeyConditionExpression=Key('pump_id').eq(1) & Key('timestamp').gte((datetime.now() - timedelta(days=1)).isoformat())\n",
    ")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(response['Items'])\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df = df.set_index('timestamp')\n",
    "\n",
    "# Select numerical columns for anomaly detection\n",
    "numerical_columns = ['temperature', 'pressure', 'flow_rate', 'vibration']\n",
    "X = df[numerical_columns]\n",
    "\n",
    "# Normalize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train an Isolation Forest model for anomaly detection\n",
    "from sklearn.ensemble import IsolationForest\n",
    "model = IsolationForest(contamination=0.1, random_state=42)\n",
    "model.fit(X_scaled)\n",
    "\n",
    "# Predict anomalies\n",
    "anomalies = model.predict(X_scaled)\n",
    "X['anomaly'] = anomalies\n",
    "\n",
    "# Save the model and scaler\n",
    "import joblib\n",
    "joblib.dump(model, 'isolation_forest_model.joblib')\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "\n",
    "# Upload model and scaler to S3\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'your-bucket-name'  # Replace with your S3 bucket name\n",
    "s3.upload_file('isolation_forest_model.joblib', bucket_name, 'models/isolation_forest_model.joblib')\n",
    "s3.upload_file('scaler.joblib', bucket_name, 'models/scaler.joblib')\n",
    "\n",
    "print(\"Model and scaler uploaded to S3.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ac6c03",
   "metadata": {},
   "source": [
    "## Step 4: Real-time Anomaly Detection with AWS Lambda\n",
    "\n",
    "Now, let's create another Lambda function to perform real-time anomaly detection on incoming sensor data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee3e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = os.environ['MODEL_BUCKET']\n",
    "\n",
    "# Download the model and scaler from S3\n",
    "s3.download_file(bucket_name, 'models/isolation_forest_model.joblib', '/tmp/model.joblib')\n",
    "s3.download_file(bucket_name, 'models/scaler.joblib', '/tmp/scaler.joblib')\n",
    "\n",
    "# Load the model and scaler\n",
    "model = joblib.load('/tmp/model.joblib')\n",
    "scaler = joblib.load('/tmp/scaler.joblib')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Parse the incoming IoT data\n",
    "    iot_data = json.loads(json.dumps(event))\n",
    "    \n",
    "    # Extract the relevant features\n",
    "    features = np.array([[\n",
    "        iot_data['temperature'],\n",
    "        iot_data['pressure'],\n",
    "        iot_data['flow_rate'],\n",
    "        iot_data['vibration']\n",
    "    ]])\n",
    "    \n",
    "    # Normalize the features\n",
    "    features_scaled = scaler.transform(features)\n",
    "    \n",
    "    # Predict anomaly\n",
    "    anomaly = model.predict(features_scaled)[0]\n",
    "    \n",
    "    # Add anomaly prediction to the data\n",
    "    iot_data['anomaly'] = int(anomaly)\n",
    "    \n",
    "    # If it's an anomaly, send an alert\n",
    "    if anomaly == -1:\n",
    "        send_alert(iot_data)\n",
    "    \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps(iot_data)\n",
    "    }\n",
    "\n",
    "def send_alert(data):\n",
    "    # Implement your alerting mechanism here (e.g., SNS, email, etc.)\n",
    "    print(f\"ALERT: Anomaly detected for pump {data['pump_id']} at {data['timestamp']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea716bc",
   "metadata": {},
   "source": [
    "Deploy this Lambda function and update the IoT Rule to trigger this function instead of the previous one.\n",
    "\n",
    "## Step 5: Visualization with Amazon QuickSight\n",
    "\n",
    "To visualize the data and anomalies in QuickSight:\n",
    "\n",
    "1. Set up an Amazon Athena table for your DynamoDB data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94f4952",
   "metadata": {
    "attributes": {
     "classes": [
      "sql"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE EXTERNAL TABLE oil_pump_sensor_data (\n",
    "    pump_id int,\n",
    "    timestamp string,\n",
    "    temperature double,\n",
    "    pressure double,\n",
    "    flow_rate double,\n",
    "    vibration double,\n",
    "    anomaly int\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION 's3://your-bucket/oil_pump_data/';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe7149",
   "metadata": {},
   "source": [
    "2. Set up a QuickSight dataset using this Athena table.\n",
    "\n",
    "3. Create a QuickSight dashboard with the following visualizations:\n",
    "   - Line chart of sensor readings over time for each pump\n",
    "   - Scatter plot of temperature vs. pressure, colored by anomaly status\n",
    "   - Bar chart of anomaly count by pump\n",
    "   - KPI indicators for current readings of each sensor\n",
    "\n",
    "## Step 6: Setting up Alerts with Amazon SNS\n",
    "\n",
    "Create an SNS topic for alerts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520bb73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "\n",
    "# Create SNS topic\n",
    "response = sns.create_topic(Name='oil_pump_alerts')\n",
    "topic_arn = response['TopicArn']\n",
    "\n",
    "# Subscribe to the topic (replace with your email)\n",
    "sns.subscribe(\n",
    "    TopicArn=topic_arn,\n",
    "    Protocol='email',\n",
    "    Endpoint='your-email@example.com'\n",
    ")\n",
    "\n",
    "print(f\"SNS topic created: {topic_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c318d30d",
   "metadata": {},
   "source": [
    "Update the `send_alert` function in the anomaly detection Lambda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3b3716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "topic_arn = 'your-sns-topic-arn'  # Replace with your SNS topic ARN\n",
    "\n",
    "def send_alert(data):\n",
    "    message = f\"ALERT: Anomaly detected for pump {data['pump_id']} at {data['timestamp']}\\n\"\n",
    "    message += f\"Sensor readings: Temperature: {data['temperature']}, Pressure: {data['pressure']}, \"\n",
    "    message += f\"Flow Rate: {data['flow_rate']}, Vibration: {data['vibration']}\"\n",
    "    \n",
    "    sns.publish(\n",
    "        TopicArn=topic_arn,\n",
    "        Message=message,\n",
    "        Subject='Oil Pump Anomaly Detected'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe52f9a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This end-to-end example demonstrates how to:\n",
    "\n",
    "1. Simulate IoT device data for oil pumps\n",
    "2. Ingest and process data using AWS IoT Core and Lambda\n",
    "3. Store data in DynamoDB\n",
    "4. Train an anomaly detection model using scikit-learn\n",
    "5. Perform real-time anomaly detection on incoming data\n",
    "6. Visualize data and anomalies using QuickSight\n",
    "7. Set up alerts for detected anomalies using SNS\n",
    "\n",
    "Key points to remember:\n",
    "\n",
    "- Replace the simulated data with real IoT device inputs in a production environment\n",
    "- Implement proper error handling and logging for all Lambda functions\n",
    "- Regularly retrain the anomaly detection model as more data becomes available\n",
    "- Set up appropriate IAM roles and permissions for all services\n",
    "- Consider using AWS IoT Greengrass for edge computing capabilities\n",
    "- Implement a more sophisticated anomaly detection model (e.g., LSTM autoencoders) for better accuracy\n",
    "\n",
    "This IIoT solution provides a foundation for predictive maintenance in the oil and refinery industry. By detecting anomalies early, companies can prevent equipment failures, reduce downtime, and optimize maintenance schedules."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
