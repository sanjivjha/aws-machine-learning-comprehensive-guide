{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2afa9317",
   "metadata": {},
   "source": [
    "# Chapter 4: Advanced AWS AI/ML Services\n",
    "\n",
    "This chapter covers more specialized and powerful AI/ML services offered by AWS. These services build upon the core services we've already explored and provide advanced capabilities for specific use cases.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- An AWS account with appropriate permissions\n",
    "- AWS CLI configured with your credentials\n",
    "- Python 3.7 or later\n",
    "- Required Python packages: boto3, pandas, numpy, matplotlib, scikit-learn\n",
    "\n",
    "Install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a807f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install boto3 pandas numpy matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bc1959",
   "metadata": {},
   "source": [
    "## 1. Amazon SageMaker Advanced Features\n",
    "\n",
    "### 1.1 SageMaker Autopilot\n",
    "\n",
    "SageMaker Autopilot automatically trains and tunes the best machine learning models for classification or regression based on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a341719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.autopilot.automl import AutoML\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load sample data\n",
    "diabetes = load_diabetes()\n",
    "X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "y = pd.DataFrame(diabetes.target, columns=['target'])\n",
    "data = pd.concat([X, y], axis=1)\n",
    "\n",
    "# Split the data\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Save data to CSV files\n",
    "train.to_csv('train.csv', index=False)\n",
    "test.to_csv('test.csv', index=False)\n",
    "\n",
    "# Set up SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker-autopilot-diabetes'\n",
    "\n",
    "# Upload data to S3\n",
    "train_s3 = sagemaker_session.upload_data('train.csv', bucket=bucket, key_prefix=prefix)\n",
    "test_s3 = sagemaker_session.upload_data('test.csv', bucket=bucket, key_prefix=prefix)\n",
    "\n",
    "# Set up Autopilot job\n",
    "auto_ml = AutoML(\n",
    "    role='your-sagemaker-role-arn',  # Replace with your SageMaker role ARN\n",
    "    target_attribute_name='target',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    output_path=f's3://{bucket}/{prefix}/output'\n",
    ")\n",
    "\n",
    "# Run Autopilot job\n",
    "auto_ml.fit(\n",
    "    inputs=train_s3,\n",
    "    job_name='diabetes-autopilot-job',\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "# Get best model\n",
    "best_model = auto_ml.best_candidate()\n",
    "print(f\"Best model: {best_model}\")\n",
    "\n",
    "# Deploy model\n",
    "predictor = auto_ml.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    candidate=best_model\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "test_data = test.drop('target', axis=1)\n",
    "predictions = predictor.predict(test_data.values)\n",
    "print(\"Sample predictions:\")\n",
    "print(predictions[:5])\n",
    "\n",
    "# Clean up\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edbde21",
   "metadata": {},
   "source": [
    "### 1.2 SageMaker Model Monitor\n",
    "\n",
    "SageMaker Model Monitor continuously monitors the quality of machine learning models in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c164e19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "# Assume you have a deployed model endpoint named 'your-model-endpoint'\n",
    "endpoint_name = 'your-model-endpoint'\n",
    "\n",
    "# Set up data capture\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=100,\n",
    "    destination_s3_uri=f's3://{bucket}/{prefix}/data-capture'\n",
    ")\n",
    "\n",
    "# Create a default model monitor\n",
    "my_monitor = DefaultModelMonitor(\n",
    "    role='your-sagemaker-role-arn',  # Replace with your SageMaker role ARN\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "# Set up the baseline\n",
    "my_monitor.suggest_baseline(\n",
    "    baseline_dataset='s3://path-to-your-baseline-dataset/baseline.csv',  # Replace with your baseline dataset\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=f's3://{bucket}/{prefix}/baseline-results',\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "# Create a monitoring schedule\n",
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "\n",
    "my_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name='my-monitoring-schedule',\n",
    "    endpoint_input=endpoint_name,\n",
    "    output_s3_uri=f's3://{bucket}/{prefix}/monitoring-output',\n",
    "    statistics=my_monitor.baseline_statistics(),\n",
    "    constraints=my_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")\n",
    "\n",
    "print(\"Model monitoring schedule created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1a8461",
   "metadata": {},
   "source": [
    "## 2. Amazon Augmented AI (A2I)\n",
    "\n",
    "Amazon A2I provides built-in human review workflows for common machine learning use cases. Here's an example of setting up a human review workflow for image moderation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50398b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "a2i = boto3.client('sagemaker-a2i-runtime')\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Assume you have created a human review workflow in the A2I console\n",
    "# and have the ARN of the flow definition\n",
    "flow_definition_arn = 'your-flow-definition-arn'  # Replace with your flow definition ARN\n",
    "\n",
    "# Sample image URL\n",
    "image_url = \"https://example.com/image-to-moderate.jpg\"\n",
    "\n",
    "# Create a human loop\n",
    "response = a2i.start_human_loop(\n",
    "    HumanLoopName='image-moderation-loop-' + str(int(time.time())),\n",
    "    FlowDefinitionArn=flow_definition_arn,\n",
    "    HumanLoopInput={\n",
    "        'InputContent': json.dumps({\n",
    "            \"initialValue\": \"EXPLICIT\",\n",
    "            \"imageUrl\": image_url\n",
    "        })\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Human loop created: {response['HumanLoopArn']}\")\n",
    "\n",
    "# In a real scenario, you would wait for the human review to complete\n",
    "# and then process the results. Here's how you might check the status:\n",
    "\n",
    "# Get human loop status\n",
    "human_loop_name = response['HumanLoopArn'].split('/')[-1]\n",
    "status_response = a2i.describe_human_loop(HumanLoopName=human_loop_name)\n",
    "print(f\"Human loop status: {status_response['HumanLoopStatus']}\")\n",
    "\n",
    "# When the loop is completed, you can get the results from S3\n",
    "# The results location would be specified in your flow definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152e89ae",
   "metadata": {},
   "source": [
    "## 3. Amazon Textract\n",
    "\n",
    "Amazon Textract is a service that automatically extracts text, handwriting, and data from scanned documents. Here's an example of using Textract to extract text and form data from a document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfced17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "textract = boto3.client('textract')\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Assume you have a document in your S3 bucket\n",
    "bucket_name = 'your-bucket-name'  # Replace with your bucket name\n",
    "document_name = 'sample-form.pdf'  # Replace with your document name\n",
    "\n",
    "# Start the Textract job\n",
    "response = textract.start_document_analysis(\n",
    "    DocumentLocation={\n",
    "        'S3Object': {\n",
    "            'Bucket': bucket_name,\n",
    "            'Name': document_name\n",
    "        }\n",
    "    },\n",
    "    FeatureTypes=['FORMS', 'TABLES']\n",
    ")\n",
    "\n",
    "job_id = response['JobId']\n",
    "\n",
    "# Wait for the job to complete\n",
    "while True:\n",
    "    response = textract.get_document_analysis(JobId=job_id)\n",
    "    status = response['JobStatus']\n",
    "    print(f\"Job status: {status}\")\n",
    "    if status in ['SUCCEEDED', 'FAILED']:\n",
    "        break\n",
    "    time.sleep(5)\n",
    "\n",
    "# Process the results\n",
    "if status == 'SUCCEEDED':\n",
    "    print(\"\\nExtracted text:\")\n",
    "    for item in response['Blocks']:\n",
    "        if item['BlockType'] == 'LINE':\n",
    "            print(item['Text'])\n",
    "    \n",
    "    print(\"\\nExtracted form data:\")\n",
    "    for item in response['Blocks']:\n",
    "        if item['BlockType'] == 'KEY_VALUE_SET':\n",
    "            if 'KEY' in item['EntityTypes']:\n",
    "                key = item['Relationships'][0]['Ids']\n",
    "                value = item['Relationships'][1]['Ids']\n",
    "                key_text = next(block['Text'] for block in response['Blocks'] if block['Id'] == key[0])\n",
    "                value_text = next(block['Text'] for block in response['Blocks'] if block['Id'] == value[0])\n",
    "                print(f\"{key_text}: {value_text}\")\n",
    "else:\n",
    "    print(\"Textract job failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d64f7",
   "metadata": {},
   "source": [
    "## 4. Amazon Kendra\n",
    "\n",
    "Amazon Kendra is an intelligent search service powered by machine learning. Here's an example of setting up a Kendra index and performing a search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d15bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "kendra = boto3.client('kendra')\n",
    "\n",
    "# Create a Kendra index\n",
    "response = kendra.create_index(\n",
    "    Name='sample-index',\n",
    "    Edition='DEVELOPER_EDITION',\n",
    "    RoleArn='your-kendra-role-arn'  # Replace with your Kendra role ARN\n",
    ")\n",
    "\n",
    "index_id = response['Id']\n",
    "\n",
    "# Wait for the index to be created\n",
    "while True:\n",
    "    response = kendra.describe_index(Id=index_id)\n",
    "    status = response['Status']\n",
    "    print(f\"Index status: {status}\")\n",
    "    if status == 'ACTIVE':\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "# Add a data source (example: S3)\n",
    "response = kendra.create_data_source(\n",
    "    IndexId=index_id,\n",
    "    Name='sample-data-source',\n",
    "    Type='S3',\n",
    "    DataSourceConfiguration={\n",
    "        'S3Configuration': {\n",
    "            'BucketName': 'your-bucket-name',  # Replace with your bucket name\n",
    "            'InclusionPrefixes': ['documents/']\n",
    "        }\n",
    "    },\n",
    "    RoleArn='your-kendra-role-arn'  # Replace with your Kendra role ARN\n",
    ")\n",
    "\n",
    "data_source_id = response['Id']\n",
    "\n",
    "# Sync the data source\n",
    "kendra.start_data_source_sync_job(\n",
    "    Id=data_source_id,\n",
    "    IndexId=index_id\n",
    ")\n",
    "\n",
    "# Perform a search\n",
    "response = kendra.query(\n",
    "    IndexId=index_id,\n",
    "    QueryText='What is machine learning?'\n",
    ")\n",
    "\n",
    "print(\"\\nSearch results:\")\n",
    "for result in response['ResultItems']:\n",
    "    print(f\"Document Title: {result.get('DocumentTitle')}\")\n",
    "    print(f\"Document Excerpt: {result.get('DocumentExcerpt', {}).get('Text')}\")\n",
    "    print(\"---\")\n",
    "\n",
    "# Clean up (uncomment to delete the index when you're done)\n",
    "# kendra.delete_index(Id=index_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d018a83e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "These advanced AWS AI/ML services provide powerful capabilities for specific use cases:\n",
    "\n",
    "1. SageMaker Autopilot for automated machine learning\n",
    "2. SageMaker Model Monitor for continuous model quality monitoring\n",
    "3. Amazon Augmented AI (A2I) for human review workflows\n",
    "4. Amazon Textract for document text and data extraction\n",
    "5. Amazon Kendra for intelligent search\n",
    "\n",
    "These services can significantly accelerate your AI/ML projects by automating complex tasks and providing specialized functionality. As you work with these services, remember to:\n",
    "\n",
    "- Manage your AWS resources carefully to control costs\n",
    "- Ensure you have the necessary permissions and IAM roles set up\n",
    "- Be aware of service limits and quotas\n",
    "- Consider data privacy and security, especially when dealing with sensitive information\n",
    "\n",
    "As you become more familiar with these advanced services, explore how they can be combined with core AI services and your existing workflows to create sophisticated, AI-powered applications."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
