{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61c0cc03",
   "metadata": {},
   "source": [
    "# Chapter 3: Core AWS AI Services - Executable Examples\n",
    "\n",
    "This chapter provides executable examples for the core AWS AI services. Each example is self-contained and can be run with minimal setup.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- An AWS account with appropriate permissions\n",
    "- AWS CLI configured with your credentials\n",
    "- Python 3.7 or later\n",
    "- Required Python packages: boto3, pandas, numpy, scikit-learn, matplotlib\n",
    "\n",
    "Install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97268cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install boto3 pandas numpy scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cc5398",
   "metadata": {},
   "source": [
    "## 1. Amazon SageMaker: Iris Flower Classification\n",
    "\n",
    "This example uses the Iris dataset to demonstrate Amazon SageMaker's capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b4ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "# Load and prepare data\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data = np.column_stack((X_train, y_train))\n",
    "test_data = np.column_stack((X_test, y_test))\n",
    "\n",
    "pd.DataFrame(train_data).to_csv('train.csv', header=False, index=False)\n",
    "pd.DataFrame(test_data).to_csv('test.csv', header=False, index=False)\n",
    "\n",
    "# Set up SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-iris'\n",
    "\n",
    "# Upload data to S3\n",
    "train_location = sagemaker_session.upload_data('train.csv', bucket=bucket, key_prefix=prefix)\n",
    "test_location = sagemaker_session.upload_data('test.csv', bucket=bucket, key_prefix=prefix)\n",
    "\n",
    "# Set up estimator\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost')\n",
    "role = get_execution_role()\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path=f's3://{bucket}/{prefix}/output',\n",
    "                                    sagemaker_session=sagemaker_session)\n",
    "\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        objective='multi:softmax',\n",
    "                        num_class=3,\n",
    "                        num_round=100)\n",
    "\n",
    "# Train the model\n",
    "xgb.fit({'train': train_location, 'validation': test_location})\n",
    "\n",
    "# Deploy the model\n",
    "predictor = xgb.deploy(initial_instance_count=1, instance_type='ml.t2.medium')\n",
    "\n",
    "# Make predictions\n",
    "test_data_np = np.loadtxt('test.csv', delimiter=',')\n",
    "test_features = test_data_np[:, :-1]\n",
    "\n",
    "predictions = predictor.predict(test_features).decode('utf-8')\n",
    "predictions = np.fromstring(predictions[1:-1], sep=',')\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Model Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions, target_names=iris.target_names))\n",
    "\n",
    "# Clean up\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43ac15d",
   "metadata": {},
   "source": [
    "## 2. Amazon Comprehend: Sentiment Analysis\n",
    "\n",
    "This example uses Amazon Comprehend to perform sentiment analysis on a sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c70713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "comprehend = boto3.client('comprehend')\n",
    "\n",
    "text = \"I love using AWS services. They make machine learning so much easier!\"\n",
    "\n",
    "# Detect sentiment\n",
    "sentiment = comprehend.detect_sentiment(Text=text, LanguageCode='en')\n",
    "print(f\"Sentiment: {sentiment['Sentiment']}\")\n",
    "print(f\"Sentiment Scores: {json.dumps(sentiment['SentimentScore'], indent=2)}\")\n",
    "\n",
    "# Detect entities\n",
    "entities = comprehend.detect_entities(Text=text, LanguageCode='en')\n",
    "print(\"\\nEntities:\")\n",
    "for entity in entities['Entities']:\n",
    "    print(f\"  {entity['Text']} ({entity['Type']})\")\n",
    "\n",
    "# Detect key phrases\n",
    "phrases = comprehend.detect_key_phrases(Text=text, LanguageCode='en')\n",
    "print(\"\\nKey Phrases:\")\n",
    "for phrase in phrases['KeyPhrases']:\n",
    "    print(f\"  {phrase['Text']}\")\n",
    "\n",
    "# Detect language\n",
    "languages = comprehend.detect_dominant_language(Text=text)\n",
    "print(\"\\nDominant Language:\")\n",
    "for language in languages['Languages']:\n",
    "    print(f\"  {language['LanguageCode']} ({language['Score']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bc7403",
   "metadata": {},
   "source": [
    "## 3. Amazon Rekognition: Object and Face Detection\n",
    "\n",
    "This example uses Amazon Rekognition to detect objects and faces in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8524a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Download a sample image\n",
    "image_url = \"https://images.pexels.com/photos/1516680/pexels-photo-1516680.jpeg\"\n",
    "response = requests.get(image_url)\n",
    "with open(\"sample_image.jpg\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Set up Rekognition client\n",
    "rekognition = boto3.client('rekognition')\n",
    "\n",
    "# Detect objects\n",
    "with open(\"sample_image.jpg\", \"rb\") as image_file:\n",
    "    response = rekognition.detect_labels(Image={'Bytes': image_file.read()})\n",
    "\n",
    "print(\"Detected objects:\")\n",
    "for label in response['Labels']:\n",
    "    print(f\"  {label['Name']} (Confidence: {label['Confidence']:.2f})\")\n",
    "\n",
    "# Detect faces\n",
    "with open(\"sample_image.jpg\", \"rb\") as image_file:\n",
    "    response = rekognition.detect_faces(Image={'Bytes': image_file.read()}, Attributes=['ALL'])\n",
    "\n",
    "print(\"\\nDetected faces:\")\n",
    "for face in response['FaceDetails']:\n",
    "    print(f\"  Age range: {face['AgeRange']['Low']}-{face['AgeRange']['High']}\")\n",
    "    print(f\"  Gender: {face['Gender']['Value']} (Confidence: {face['Gender']['Confidence']:.2f})\")\n",
    "    print(f\"  Emotions: {face['Emotions'][0]['Type']} (Confidence: {face['Emotions'][0]['Confidence']:.2f})\")\n",
    "\n",
    "# Draw bounding boxes\n",
    "image = Image.open(\"sample_image.jpg\")\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "for face in response['FaceDetails']:\n",
    "    box = face['BoundingBox']\n",
    "    left = image.width * box['Left']\n",
    "    top = image.height * box['Top']\n",
    "    width = image.width * box['Width']\n",
    "    height = image.height * box['Height']\n",
    "    draw.rectangle([left, top, left + width, top + height], outline='red', width=2)\n",
    "\n",
    "image.save(\"output_image.jpg\")\n",
    "print(\"\\nImage with bounding boxes saved as 'output_image.jpg'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4271197",
   "metadata": {},
   "source": [
    "## 4. Amazon Forecast: Time Series Forecasting\n",
    "\n",
    "This example demonstrates how to use Amazon Forecast for time series prediction. Note that this is a simplified example and actual forecast creation takes more time and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad7a383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Create sample time series data\n",
    "start_date = datetime(2022, 1, 1)\n",
    "dates = [start_date + timedelta(days=i) for i in range(365)]\n",
    "values = np.sin(np.arange(365) * 2 * np.pi / 365) * 50 + 100 + np.random.normal(0, 10, 365)\n",
    "df = pd.DataFrame({'date': dates, 'value': values})\n",
    "df['date'] = df['date'].dt.strftime('%Y-%m-%d')\n",
    "df.to_csv('time_series_data.csv', index=False)\n",
    "\n",
    "# Set up Forecast client\n",
    "forecast = boto3.client('forecast')\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Upload data to S3\n",
    "bucket_name = 'your-bucket-name'  # Replace with your S3 bucket name\n",
    "s3.upload_file('time_series_data.csv', bucket_name, 'forecast_data/time_series_data.csv')\n",
    "\n",
    "# Create dataset group\n",
    "response = forecast.create_dataset_group(\n",
    "    DatasetGroupName='sample-forecast-dataset-group',\n",
    "    Domain='CUSTOM',\n",
    "    DatasetArns=[]\n",
    ")\n",
    "dataset_group_arn = response['DatasetGroupArn']\n",
    "\n",
    "# Create dataset\n",
    "response = forecast.create_dataset(\n",
    "    DatasetName='sample-forecast-dataset',\n",
    "    Domain='CUSTOM',\n",
    "    DatasetType='TARGET_TIME_SERIES',\n",
    "    DataFrequency='D',\n",
    "    Schema={\n",
    "        'Attributes': [\n",
    "            {'AttributeName': 'date', 'AttributeType': 'timestamp'},\n",
    "            {'AttributeName': 'value', 'AttributeType': 'float'},\n",
    "            {'AttributeName': 'item_id', 'AttributeType': 'string'}\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "dataset_arn = response['DatasetArn']\n",
    "\n",
    "# Create dataset import job\n",
    "response = forecast.create_dataset_import_job(\n",
    "    DatasetImportJobName='sample-import-job',\n",
    "    DatasetArn=dataset_arn,\n",
    "    DataSource={\n",
    "        'S3Config': {\n",
    "            'Path': f's3://{bucket_name}/forecast_data/time_series_data.csv',\n",
    "            'RoleArn': 'your-role-arn'  # Replace with your IAM role ARN\n",
    "        }\n",
    "    },\n",
    "    TimestampFormat='yyyy-MM-dd'\n",
    ")\n",
    "\n",
    "# Wait for import job to complete (this may take several minutes)\n",
    "import time\n",
    "while True:\n",
    "    response = forecast.describe_dataset_import_job(\n",
    "        DatasetImportJobArn=response['DatasetImportJobArn']\n",
    "    )\n",
    "    if response['Status'] in ['ACTIVE', 'CREATE_FAILED']:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "print(\"Dataset import job status:\", response['Status'])\n",
    "\n",
    "# Note: In a real scenario, you would proceed to create a predictor and forecast\n",
    "# However, these steps can take significant time and resources\n",
    "print(\"To create a predictor and forecast, use the following steps:\")\n",
    "print(\"1. forecast.create_predictor()\")\n",
    "print(\"2. forecast.create_forecast()\")\n",
    "print(\"3. forecast.query_forecast()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9645ce55",
   "metadata": {},
   "source": [
    "## 5. Amazon Personalize: Recommendation System\n",
    "\n",
    "This example demonstrates how to set up and use Amazon Personalize for a simple recommendation system. Note that this is a simplified example and actual model training takes more time and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05ee88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "# Create sample interaction data\n",
    "interactions = pd.DataFrame({\n",
    "    'USER_ID': ['user1', 'user1', 'user2', 'user2', 'user3'],\n",
    "    'ITEM_ID': ['item1', 'item2', 'item2', 'item3', 'item1'],\n",
    "    'TIMESTAMP': [1625097600, 1625184000, 1625270400, 1625356800, 1625443200]\n",
    "})\n",
    "interactions.to_csv('interactions.csv', index=False)\n",
    "\n",
    "# Set up Personalize client\n",
    "personalize = boto3.client('personalize')\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Upload data to S3\n",
    "bucket_name = 'your-bucket-name'  # Replace with your S3 bucket name\n",
    "s3.upload_file('interactions.csv', bucket_name, 'personalize_data/interactions.csv')\n",
    "\n",
    "# Create dataset group\n",
    "response = personalize.create_dataset_group(name='sample-dataset-group')\n",
    "dataset_group_arn = response['datasetGroupArn']\n",
    "\n",
    "# Create schema\n",
    "schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Interactions\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"USER_ID\", \"type\": \"string\"},\n",
    "        {\"name\": \"ITEM_ID\", \"type\": \"string\"},\n",
    "        {\"name\": \"TIMESTAMP\", \"type\": \"long\"}\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "create_schema_response = personalize.create_schema(\n",
    "    name='sample-interactions-schema',\n",
    "    schema=json.dumps(schema)\n",
    ")\n",
    "schema_arn = create_schema_response['schemaArn']\n",
    "\n",
    "# Create dataset\n",
    "create_dataset_response = personalize.create_dataset(\n",
    "    name='sample-interactions-dataset',\n",
    "    schemaArn=schema_arn,\n",
    "    datasetGroupArn=dataset_group_arn,\n",
    "    datasetType='Interactions'\n",
    ")\n",
    "dataset_arn = create_dataset_response['datasetArn']\n",
    "\n",
    "# Create dataset import job\n",
    "create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName='sample-import-job',\n",
    "    datasetArn=dataset_arn,\n",
    "    dataSource={'dataLocation': f's3://{bucket_name}/personalize_data/interactions.csv'},\n",
    "    roleArn='your-role-arn'  # Replace with your IAM role ARN\n",
    ")\n",
    "\n",
    "# Wait for import job to complete (this may take several minutes)\n",
    "import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "while True:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn=import_job_arn\n",
    "    )\n",
    "    status = describe_dataset_import_job_response['datasetImportJob']['status']\n",
    "    print(f\"Import job status: {status}\")\n",
    "    if status in ['ACTIVE', 'CREATE FAILED']:\n",
    "        break\n",
    "    sleep(30)\n",
    "\n",
    "print(\"Dataset import job completed.\")\n",
    "\n",
    "# Note: In a real scenario, you would proceed to create a solution, campaign, and get recommendations\n",
    "# However, these steps can take significant time and resources\n",
    "print(\"To create a solution, campaign, and get recommendations, use the following steps:\")\n",
    "print(\"1. personalize.create_solution()\")\n",
    "print(\"2. personalize.create_solution_version()\")\n",
    "print(\"3. personalize.create_campaign()\")\n",
    "print(\"4. personalize_runtime.get_recommendations()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d7bc0",
   "metadata": {},
   "source": [
    "## 6. Amazon Polly: Text-to-Speech\n",
    "\n",
    "Amazon Polly is a service that turns text into lifelike speech. This example demonstrates how to use Polly to synthesize speech from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d944ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from contextlib import closing\n",
    "\n",
    "polly_client = boto3.client('polly')\n",
    "\n",
    "text = \"Hello, this is a test of Amazon Polly. Isn't text-to-speech amazing?\"\n",
    "\n",
    "response = polly_client.synthesize_speech(\n",
    "    Text=text,\n",
    "    OutputFormat='mp3',\n",
    "    VoiceId='Joanna'\n",
    ")\n",
    "\n",
    "# Save the audio stream to a file\n",
    "if \"AudioStream\" in response:\n",
    "    with closing(response[\"AudioStream\"]) as stream:\n",
    "        output = os.path.join(\".\", \"speech.mp3\")\n",
    "        with open(output, \"wb\") as file:\n",
    "            file.write(stream.read())\n",
    "    print(f\"Speech saved to {output}\")\n",
    "\n",
    "# To play the audio, you can use a library like playsound\n",
    "# pip install playsound\n",
    "try:\n",
    "    from playsound import playsound\n",
    "    playsound('speech.mp3')\n",
    "except ImportError:\n",
    "    print(\"To play the audio, install playsound: pip install playsound\")\n",
    "\n",
    "# List available voices\n",
    "voices = polly_client.describe_voices()\n",
    "print(\"\\nAvailable voices:\")\n",
    "for voice in voices['Voices'][:5]:  # Printing first 5 for brevity\n",
    "    print(f\"Name: {voice['Name']}, Gender: {voice['Gender']}, Language: {voice['LanguageCode']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce260ef8",
   "metadata": {},
   "source": [
    "## 7. Amazon Transcribe: Speech-to-Text\n",
    "\n",
    "Amazon Transcribe is an automatic speech recognition service. This example shows how to transcribe an audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72137f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import urllib.request\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "transcribe_client = boto3.client('transcribe')\n",
    "\n",
    "# Download a sample audio file\n",
    "audio_url = \"https://github.com/OpenMindClub/omnivore/raw/main/audios/turing_test_audio.mp3\"\n",
    "urllib.request.urlretrieve(audio_url, \"sample_audio.mp3\")\n",
    "\n",
    "# Upload the audio file to S3\n",
    "bucket_name = 'your-bucket-name'  # Replace with your S3 bucket name\n",
    "s3_client.upload_file('sample_audio.mp3', bucket_name, 'sample_audio.mp3')\n",
    "\n",
    "# Start transcription job\n",
    "job_name = 'sample-transcription-job'\n",
    "job_uri = f's3://{bucket_name}/sample_audio.mp3'\n",
    "\n",
    "transcribe_client.start_transcription_job(\n",
    "    TranscriptionJobName=job_name,\n",
    "    Media={'MediaFileUri': job_uri},\n",
    "    MediaFormat='mp3',\n",
    "    LanguageCode='en-US'\n",
    ")\n",
    "\n",
    "# Wait for the job to complete\n",
    "while True:\n",
    "    status = transcribe_client.get_transcription_job(TranscriptionJobName=job_name)\n",
    "    if status['TranscriptionJob']['TranscriptionJobStatus'] in ['COMPLETED', 'FAILED']:\n",
    "        break\n",
    "    print(\"Not ready yet...\")\n",
    "    time.sleep(5)\n",
    "\n",
    "if status['TranscriptionJob']['TranscriptionJobStatus'] == 'COMPLETED':\n",
    "    response = urllib.request.urlopen(status['TranscriptionJob']['Transcript']['TranscriptFileUri'])\n",
    "    data = response.read()\n",
    "    text = data.decode('utf-8')\n",
    "    print(\"\\nTranscription:\")\n",
    "    print(text[:500] + \"...\")  # Printing first 500 characters for brevity\n",
    "else:\n",
    "    print(\"Transcription failed.\")\n",
    "\n",
    "# Clean up\n",
    "transcribe_client.delete_transcription_job(TranscriptionJobName=job_name)\n",
    "s3_client.delete_object(Bucket=bucket_name, Key='sample_audio.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bbeb9d",
   "metadata": {},
   "source": [
    "## 8. Amazon Translate: Language Translation\n",
    "\n",
    "Amazon Translate is a neural machine translation service. This example demonstrates how to translate text between languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e40294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "translate_client = boto3.client('translate')\n",
    "\n",
    "text = \"Hello, world. How are you doing today?\"\n",
    "\n",
    "# Translate to Spanish\n",
    "response = translate_client.translate_text(\n",
    "    Text=text,\n",
    "    SourceLanguageCode='en',\n",
    "    TargetLanguageCode='es'\n",
    ")\n",
    "\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Translated text: {response['TranslatedText']}\")\n",
    "\n",
    "# Detect language\n",
    "response = translate_client.translate_text(\n",
    "    Text=text,\n",
    "    SourceLanguageCode='auto',\n",
    "    TargetLanguageCode='de'\n",
    ")\n",
    "\n",
    "print(f\"\\nDetected source language: {response['SourceLanguageCode']}\")\n",
    "print(f\"Translated to German: {response['TranslatedText']}\")\n",
    "\n",
    "# List supported languages\n",
    "response = translate_client.list_languages()\n",
    "print(\"\\nSupported languages:\")\n",
    "for language in response['Languages'][:10]:  # Printing first 10 for brevity\n",
    "    print(f\"Language Code: {language['LanguageCode']}, Name: {language['LanguageName']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be0bc7c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "These examples demonstrate how to use the core AWS AI services:\n",
    "\n",
    "1. Amazon SageMaker for machine learning model training and deployment\n",
    "2. Amazon Comprehend for natural language processing\n",
    "3. Amazon Rekognition for image and video analysis\n",
    "4. Amazon Forecast for time series forecasting\n",
    "5. Amazon Personalize for building recommendation systems\n",
    "6. Amazon Polly for text-to-speech conversion\n",
    "7. Amazon Transcribe for speech-to-text conversion\n",
    "8. Amazon Translate for language translation\n",
    "\n",
    "\n",
    "Remember:\n",
    "- Always clean up your resources to avoid unnecessary charges.\n",
    "- These examples use small datasets for demonstration. Real-world applications typically involve larger datasets and more complex workflows.\n",
    "- Ensure you have the necessary AWS permissions to run these examples.\n",
    "- Some services (like Forecast and Personalize) may take significant time to process data and train models in real-world scenarios.\n",
    "\n",
    "As you become more familiar with these services, explore their advanced features and integrate them into your AI/ML projects."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
