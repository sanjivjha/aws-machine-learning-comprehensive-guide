{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28453cd1",
   "metadata": {},
   "source": [
    "# End-to-End Customer Churn Prediction on AWS\n",
    "\n",
    "This case study demonstrates a complete workflow for predicting customer churn using AWS services. We'll use a telecommunications customer dataset to predict which customers are likely to cancel their service.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- An AWS account with appropriate permissions\n",
    "- AWS CLI configured with your credentials\n",
    "- Python 3.7 or later\n",
    "- Required Python packages: boto3, sagemaker, pandas, numpy, matplotlib, seaborn\n",
    "\n",
    "Install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2407b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install boto3 sagemaker pandas numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788bbc20",
   "metadata": {},
   "source": [
    "## Step 1: Data Preparation\n",
    "\n",
    "First, we'll create a sample dataset and upload it to S3. In a real-world scenario, you would replace this with your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4b762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "# Create sample data\n",
    "np.random.seed(42)\n",
    "n_customers = 1000\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'customer_id': range(1, n_customers + 1),\n",
    "    'tenure': np.random.randint(1, 72, n_customers),\n",
    "    'monthly_charges': np.random.uniform(20, 100, n_customers),\n",
    "    'total_charges': np.random.uniform(100, 5000, n_customers),\n",
    "    'contract': np.random.choice(['Month-to-month', 'One year', 'Two year'], n_customers),\n",
    "    'online_security': np.random.choice(['No', 'Yes'], n_customers),\n",
    "    'tech_support': np.random.choice(['No', 'Yes'], n_customers),\n",
    "    'streaming_tv': np.random.choice(['No', 'Yes'], n_customers),\n",
    "    'streaming_movies': np.random.choice(['No', 'Yes'], n_customers),\n",
    "    'churn': np.random.choice([0, 1], n_customers, p=[0.7, 0.3])  # 30% churn rate\n",
    "})\n",
    "\n",
    "# Calculate total_charges based on tenure and monthly_charges\n",
    "data['total_charges'] = data['tenure'] * data['monthly_charges']\n",
    "\n",
    "# Upload to S3\n",
    "bucket_name = 'your-bucket-name'  # Replace with your S3 bucket name\n",
    "key = 'churn_data/telco_churn.csv'\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "csv_buffer = io.StringIO()\n",
    "data.to_csv(csv_buffer, index=False)\n",
    "s3.put_object(Bucket=bucket_name, Key=key, Body=csv_buffer.getvalue())\n",
    "\n",
    "print(f\"Data uploaded to s3://{bucket_name}/{key}\")\n",
    "\n",
    "# To use your own data, comment out the code above and use:\n",
    "# s3.upload_file('path/to/your/data.csv', bucket_name, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb59f87",
   "metadata": {},
   "source": [
    "## Step 2: Data Exploration and Preprocessing\n",
    "\n",
    "We'll use SageMaker to explore and preprocess our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21930d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "sagemaker_session = Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Create a ScriptProcessor\n",
    "processor = ScriptProcessor(\n",
    "    command=['python3'],\n",
    "    image_uri=sagemaker.image_uris.retrieve(\n",
    "        framework=\"sklearn\",\n",
    "        region=sagemaker_session.boto_region_name,\n",
    "        version=\"0.23-1\"),\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge'\n",
    ")\n",
    "\n",
    "# Run the processing job\n",
    "processor.run(\n",
    "    code='preprocess.py',\n",
    "    inputs=[ProcessingInput(\n",
    "        source=f's3://{bucket_name}/{key}',\n",
    "        destination='/opt/ml/processing/input'\n",
    "    )],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name='train', source='/opt/ml/processing/train'),\n",
    "        ProcessingOutput(output_name='validation', source='/opt/ml/processing/validation'),\n",
    "        ProcessingOutput(output_name='test', source='/opt/ml/processing/test')\n",
    "    ],\n",
    "    arguments=['--input-data', '/opt/ml/processing/input/telco_churn.csv']\n",
    ")\n",
    "\n",
    "print(\"Data preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2b0398",
   "metadata": {},
   "source": [
    "Create a file named `preprocess.py` with the following content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b36a7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def preprocess_telco_data(df):\n",
    "    # Convert categorical variables to numeric\n",
    "    df['contract'] = pd.Categorical(df['contract']).codes\n",
    "    df['online_security'] = pd.Categorical(df['online_security']).codes\n",
    "    df['tech_support'] = pd.Categorical(df['tech_support']).codes\n",
    "    df['streaming_tv'] = pd.Categorical(df['streaming_tv']).codes\n",
    "    df['streaming_movies'] = pd.Categorical(df['streaming_movies']).codes\n",
    "\n",
    "    # Separate features and target\n",
    "    X = df.drop(['customer_id', 'churn'], axis=1)\n",
    "    y = df['churn']\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns), y\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input-data', type=str, required=True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Read input data\n",
    "    input_data = pd.read_csv(args.input_data)\n",
    "    print('Shape of input data:', input_data.shape)\n",
    "\n",
    "    # Preprocess data\n",
    "    X, y = preprocess_telco_data(input_data)\n",
    "    print('Shape of preprocessed features:', X.shape)\n",
    "\n",
    "    # Split data into train, validation, and test sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Save preprocessed datasets\n",
    "    pd.concat([X_train, y_train], axis=1).to_csv('/opt/ml/processing/train/train.csv', index=False)\n",
    "    pd.concat([X_val, y_val], axis=1).to_csv('/opt/ml/processing/validation/validation.csv', index=False)\n",
    "    pd.concat([X_test, y_test], axis=1).to_csv('/opt/ml/processing/test/test.csv', index=False)\n",
    "\n",
    "    print('Preprocessing completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5838a65",
   "metadata": {},
   "source": [
    "## Step 3: Model Training\n",
    "\n",
    "We'll use SageMaker's built-in XGBoost algorithm to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e91cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.xgboost import XGBoost\n",
    "\n",
    "# Set up the estimator\n",
    "xgb = XGBoost(\n",
    "    entry_point='train.py',\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    framework_version='1.0-1',\n",
    "    output_path=f's3://{bucket_name}/model_output',\n",
    "    hyperparameters={\n",
    "        'max_depth': 5,\n",
    "        'eta': 0.2,\n",
    "        'gamma': 4,\n",
    "        'min_child_weight': 6,\n",
    "        'subsample': 0.8,\n",
    "        'objective': 'binary:logistic',\n",
    "        'num_round': 100\n",
    "    }\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb.fit({\n",
    "    'train': f's3://{bucket_name}/churn_data/train',\n",
    "    'validation': f's3://{bucket_name}/churn_data/validation'\n",
    "})\n",
    "\n",
    "print(\"Model training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22765024",
   "metadata": {},
   "source": [
    "Create a file named `train.py` with the following content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b41a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "if __name__ =='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # Hyperparameters are described here\n",
    "    parser.add_argument('--num_round', type=int, default=999)\n",
    "    parser.add_argument('--max_depth', type=int, default=3)\n",
    "    parser.add_argument('--eta', type=float, default=0.1)\n",
    "    \n",
    "    # SageMaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--validation', type=str, default=os.environ.get('SM_CHANNEL_VALIDATION'))\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    train = pd.read_csv(f'{args.train}/train.csv')\n",
    "    validation = pd.read_csv(f'{args.validation}/validation.csv')\n",
    "    \n",
    "    X_train = train.drop('churn', axis=1)\n",
    "    y_train = train['churn']\n",
    "    X_validation = validation.drop('churn', axis=1)\n",
    "    y_validation = validation['churn']\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalidation = xgb.DMatrix(X_validation, label=y_validation)\n",
    "    \n",
    "    params = {\n",
    "        'max_depth': args.max_depth,\n",
    "        'eta': args.eta,\n",
    "        'objective': 'binary:logistic'\n",
    "    }\n",
    "    \n",
    "    model = xgb.train(params, dtrain, args.num_round, evals=[(dvalidation, 'validation')])\n",
    "    \n",
    "    model.save_model(f'{args.model_dir}/xgboost-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc15d950",
   "metadata": {},
   "source": [
    "## Step 4: Model Deployment\n",
    "\n",
    "Now, let's deploy our trained model to a SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fb9fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = xgb.deploy(initial_instance_count=1, instance_type='ml.t2.medium')\n",
    "\n",
    "print(f\"Model deployed. Endpoint name: {predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e45a9a",
   "metadata": {},
   "source": [
    "## Step 5: Inference and Evaluation\n",
    "\n",
    "We'll use our deployed model to make predictions on the test set and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bf79d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load test data\n",
    "test_data = pd.read_csv(f's3://{bucket_name}/churn_data/test/test.csv')\n",
    "X_test = test_data.drop('churn', axis=1)\n",
    "y_test = test_data['churn']\n",
    "\n",
    "# Make predictions\n",
    "predictions = predictor.predict(X_test.values)\n",
    "\n",
    "# Convert raw predictions to binary predictions\n",
    "y_pred = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc_roc = roc_auc_score(y_test, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "\n",
    "# Visualize feature importance\n",
    "feature_importance = predictor.predict(X_test.values, initial_args={'pred_contribs': 'True'})\n",
    "feature_names = X_test.columns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=np.mean(feature_importance, axis=0), y=feature_names)\n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('Mean SHAP Value')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot to S3\n",
    "img_data = io.BytesIO()\n",
    "plt.savefig(img_data, format='png')\n",
    "img_data.seek(0)\n",
    "s3.put_object(Body=img_data, Bucket=bucket_name, Key='churn_prediction/feature_importance.png')\n",
    "\n",
    "print(f\"Feature importance plot saved to s3://{bucket_name}/churn_prediction/feature_importance.png\")\n",
    "\n",
    "# Clean up\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9166b2f1",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This end-to-end example demonstrates how to:\n",
    "\n",
    "1. Prepare and upload data to S3\n",
    "2. Preprocess data using SageMaker Processing\n",
    "3. Train a model using SageMaker's built-in XGBoost algorithm\n",
    "4. Deploy the trained model to a SageMaker endpoint\n",
    "5. Make predictions and evaluate the model's performance\n",
    "6. Visualize feature importance and save the plot to S3\n",
    "\n",
    "Key points to remember:\n",
    "\n",
    "- Replace the sample data with your own dataset for real-world applications\n",
    "- Adjust hyperparameters and model architecture based on your specific use case\n",
    "- Consider using SageMaker Experiments to track multiple training runs\n",
    "- Implement proper error handling and logging for production environments\n",
    "- Set up monitoring for the deployed model to track its performance over time\n",
    "\n",
    "By following this workflow, you can build and deploy machine learning models on AWS for various business problems, leveraging the scalability and managed services provided by the AWS ecosystem."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
