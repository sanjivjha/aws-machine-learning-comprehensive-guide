{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd5e7700",
   "metadata": {},
   "source": [
    "# Chapter 8: Generative AI on AWS\n",
    "\n",
    "## 8.1 Introduction to Generative AI\n",
    "\n",
    "Generative AI refers to artificial intelligence systems that can create new content, including text, images, code, and more. AWS offers several services and tools to leverage generative AI capabilities, enabling developers to build sophisticated AI-powered applications.\n",
    "\n",
    "## 8.2 Amazon Bedrock\n",
    "\n",
    "Amazon Bedrock is a fully managed service that provides access to high-performing foundation models (FMs) from leading AI companies through a single API.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "- Access to various foundation models (e.g., Claude from Anthropic, Jurassic-2 from AI21 Labs)\n",
    "- Customization capabilities through fine-tuning\n",
    "- Serverless experience with pay-as-you-go pricing\n",
    "- Enterprise-grade security and privacy\n",
    "\n",
    "**Example: Text Generation with Bedrock**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0179751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "bedrock = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "prompt = \"Write a short story about a robot learning to paint:\"\n",
    "\n",
    "body = json.dumps({\n",
    "    \"prompt\": prompt,\n",
    "    \"max_tokens_to_sample\": 300,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.8,\n",
    "})\n",
    "\n",
    "modelId = 'anthropic.claude-v2'\n",
    "accept = 'application/json'\n",
    "contentType = 'application/json'\n",
    "\n",
    "response = bedrock.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "response_body = json.loads(response.get('body').read())\n",
    "\n",
    "print(response_body.get('completion'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d29430",
   "metadata": {},
   "source": [
    "## 8.3 Amazon SageMaker JumpStart\n",
    "\n",
    "SageMaker JumpStart provides pre-trained, open-source models for a wide variety of problem types to help you get started with machine learning.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "- One-click deployment of pre-trained models\n",
    "- Fine-tuning capabilities\n",
    "- Integration with SageMaker's managed infrastructure\n",
    "\n",
    "**Example: Deploying a JumpStart Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f643d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "role = get_execution_role()\n",
    "region = sagemaker.Session().boto_region_name\n",
    "\n",
    "model_id = \"huggingface-text2text-flan-t5-xl\"\n",
    "model = JumpStartModel(model_id=model_id)\n",
    "\n",
    "predictor = model.deploy()\n",
    "\n",
    "result = predictor.predict(\"Translate to French: Hello, how are you?\")\n",
    "print(result)\n",
    "\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196d82e7",
   "metadata": {},
   "source": [
    "## 8.4 Retrieval-Augmented Generation (RAG) Application\n",
    "\n",
    "RAG combines the power of large language models with a retrieval system to generate more accurate and contextually relevant responses. Let's build a RAG application using Amazon Bedrock and Amazon Kendra.\n",
    "\n",
    "### 8.4.1 Setting Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fa9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "kendra = boto3.client('kendra')\n",
    "bedrock = boto3.client('bedrock-runtime')\n",
    "\n",
    "bucket_name = 'your-bucket-name'\n",
    "kendra_role_arn = 'arn:aws:iam::your-account-id:role/KendraRoleWithRequiredPermissions'\n",
    "s3_access_role_arn = 'arn:aws:iam::your-account-id:role/KendraRoleWithS3Access'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a31db2",
   "metadata": {},
   "source": [
    "### 8.4.2 Indexing Documents with Kendra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1e20dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_upload_pdf():\n",
    "    pdf_url = \"https://docs.aws.amazon.com/pdfs/whitepapers/latest/aws-overview/aws-overview.pdf\"\n",
    "    pdf_name = \"aws-overview.pdf\"\n",
    "    \n",
    "    response = requests.get(pdf_url)\n",
    "    with open(pdf_name, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    \n",
    "    s3.upload_file(pdf_name, bucket_name, f\"documents/{pdf_name}\")\n",
    "    print(f\"AWS Overview PDF uploaded to s3://{bucket_name}/documents/{pdf_name}\")\n",
    "    \n",
    "    os.remove(pdf_name)\n",
    "\n",
    "def create_kendra_index():\n",
    "    response = kendra.create_index(\n",
    "        Name='AWSOverviewIndex',\n",
    "        Edition='DEVELOPER_EDITION',\n",
    "        RoleArn=kendra_role_arn\n",
    "    )\n",
    "    index_id = response['Id']\n",
    "    \n",
    "    while True:\n",
    "        response = kendra.describe_index(Id=index_id)\n",
    "        if response['Status'] == 'ACTIVE':\n",
    "            break\n",
    "        time.sleep(60)\n",
    "    \n",
    "    print(f\"Kendra Index created with ID: {index_id}\")\n",
    "    return index_id\n",
    "\n",
    "def create_kendra_datasource(index_id):\n",
    "    response = kendra.create_data_source(\n",
    "        IndexId=index_id,\n",
    "        Name='AWSOverviewDataSource',\n",
    "        Type='S3',\n",
    "        DataSourceConfiguration={\n",
    "            'S3Configuration': {\n",
    "                'BucketName': bucket_name,\n",
    "                'InclusionPrefixes': ['documents/aws-overview.pdf']\n",
    "            }\n",
    "        },\n",
    "        RoleArn=s3_access_role_arn\n",
    "    )\n",
    "    data_source_id = response['Id']\n",
    "    \n",
    "    kendra.start_data_source_sync_job(Id=data_source_id, IndexId=index_id)\n",
    "    \n",
    "    while True:\n",
    "        sync_status = kendra.describe_data_source(Id=data_source_id, IndexId=index_id)\n",
    "        if sync_status['Status'] == 'ACTIVE':\n",
    "            break\n",
    "        time.sleep(60)\n",
    "    \n",
    "    print(\"AWS Overview PDF indexed in Kendra\")\n",
    "    return data_source_id\n",
    "\n",
    "# Execute the setup\n",
    "download_and_upload_pdf()\n",
    "index_id = create_kendra_index()\n",
    "data_source_id = create_kendra_datasource(index_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07763758",
   "metadata": {},
   "source": [
    "### 8.4.3 Implementing the RAG Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ffd834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_kendra(query, index_id):\n",
    "    response = kendra.query(\n",
    "        IndexId=index_id,\n",
    "        QueryText=query\n",
    "    )\n",
    "    return response['ResultItems']\n",
    "\n",
    "def generate_bedrock_response(query, context):\n",
    "    prompt = f\"\"\"Human: You are an AI assistant with knowledge about AWS services. Use the following information from the AWS Overview whitepaper to answer the human's question. If the information doesn't contain the answer, say you don't know but provide general information about AWS if relevant.\n",
    "\n",
    "Information:\n",
    "{context}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ec6e5d",
   "metadata": {},
   "source": [
    "To use the RAG application and chat with it, you would typically create a function that combines the Kendra querying and Bedrock response generation. Here's an example of how you might implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810ec7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chat(query, index_id):\n",
    "    # Query Kendra for relevant information\n",
    "    kendra_results = query_kendra(query, index_id)\n",
    "    \n",
    "    # Extract and combine the relevant text from Kendra results\n",
    "    context = \"\\n\".join([result['DocumentExcerpt']['Text'] for result in kendra_results[:3]])\n",
    "    \n",
    "    # Generate a response using Bedrock\n",
    "    bedrock_response = generate_bedrock_response(query, context)\n",
    "    \n",
    "    return bedrock_response\n",
    "\n",
    "# Example usage\n",
    "index_id = \"your-kendra-index-id\"  # Replace with your actual Kendra index ID\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in ['exit', 'quit', 'bye']:\n",
    "        print(\"AI: Goodbye! Have a great day!\")\n",
    "        break\n",
    "    \n",
    "    response = rag_chat(user_input, index_id)\n",
    "    print(\"AI:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e3db95",
   "metadata": {},
   "source": [
    "In this implementation:\n",
    "\n",
    "1. The `rag_chat` function takes a user query and the Kendra index ID as input.\n",
    "2. It first queries Kendra to retrieve relevant information from the indexed documents.\n",
    "3. It then extracts the text from the top 3 Kendra results and combines them into a context string.\n",
    "4. This context, along with the original query, is sent to the Bedrock model to generate a response.\n",
    "5. The generated response is returned.\n",
    "\n",
    "The example usage shows how you might set up a simple chat loop:\n",
    "\n",
    "1. It continually prompts the user for input.\n",
    "2. It sends each user input to the `rag_chat` function.\n",
    "3. It prints the AI's response.\n",
    "4. The loop continues until the user types 'exit', 'quit', or 'bye'.\n",
    "\n",
    "To use this RAG application, you would run this script and start chatting. The AI would use the information from your indexed documents (in this case, the AWS Overview whitepaper) to inform its responses, providing more accurate and contextually relevant information about AWS services.\n",
    "\n",
    "Remember to replace \"your-kendra-index-id\" with the actual ID of the Kendra index you created earlier in the setup process.\n",
    "\n",
    "This implementation allows for a conversational interface with the RAG system, where users can ask questions and receive responses based on the indexed document and the capabilities of the language model.\n",
    "\n",
    "\n",
    "## 8.5 Comparing SageMaker JumpStart and Amazon Bedrock\n",
    "\n",
    "### SageMaker JumpStart\n",
    "\n",
    "SageMaker JumpStart provides a wide variety of pre-trained models that can be easily deployed or fine-tuned.\n",
    "\n",
    "Sample list of models available through JumpStart:\n",
    "\n",
    "1. Text Processing:\n",
    "   - BERT (various versions)\n",
    "   - RoBERTa\n",
    "   - DistilBERT\n",
    "   - XLM\n",
    "2. Image Processing:\n",
    "   - ResNet (various versions)\n",
    "   - MobileNet\n",
    "   - YOLOv5\n",
    "3. Tabular Data:\n",
    "   - XGBoost\n",
    "   - LightGBM\n",
    "   - CatBoost\n",
    "4. Generative AI:\n",
    "   - GPT-2\n",
    "   - T5\n",
    "\n",
    "### Amazon Bedrock\n",
    "\n",
    "Bedrock provides access to foundation models from leading AI companies.\n",
    "\n",
    "Sample list of models available through Bedrock:\n",
    "\n",
    "1. Anthropic:\n",
    "   - Claude (various versions)\n",
    "2. AI21 Labs:\n",
    "   - Jurassic-2 (various versions)\n",
    "3. Amazon:\n",
    "   - Titan (various versions)\n",
    "4. Stability AI:\n",
    "   - Stable Diffusion (for image generation)\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "1. Model Source:\n",
    "   - JumpStart: Mostly open-source models\n",
    "   - Bedrock: Proprietary models from AI companies\n",
    "2. Customization:\n",
    "   - JumpStart: Allows fine-tuning of models\n",
    "   - Bedrock: Offers customization through prompt engineering and fine-tuning (for some models)\n",
    "3. Deployment:\n",
    "   - JumpStart: Models are deployed to SageMaker endpoints\n",
    "   - Bedrock: Serverless access to models through API calls\n",
    "4. Use Cases:\n",
    "   - JumpStart: Wide range of ML tasks (classification, regression, NLP, computer vision)\n",
    "   - Bedrock: Primarily focused on large language models and generative AI\n",
    "5. Integration:\n",
    "   - JumpStart: Tightly integrated with SageMaker ecosystem\n",
    "   - Bedrock: Can be easily integrated into any application\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "Use SageMaker JumpStart when:\n",
    "- You need a wide variety of ML models for different tasks\n",
    "- You want to fine-tune models on your specific dataset\n",
    "- You're already using the SageMaker ecosystem\n",
    "- You need more control over the deployment infrastructure\n",
    "\n",
    "Use Amazon Bedrock when:\n",
    "- You need state-of-the-art large language models\n",
    "- You want a serverless solution with minimal infrastructure management\n",
    "- Your use case requires the latest generative AI capabilities\n",
    "- You need enterprise-grade security and compliance features\n",
    "\n",
    "In many cases, you might use both services in your AI/ML pipeline. For example, you could use JumpStart for data preprocessing or feature extraction, and then use Bedrock for generating human-like text based on those features.\n",
    "\n",
    "## 8.6 Example: Combining JumpStart and Bedrock in a Pipeline\n",
    "\n",
    "Here's a simple example of how you might combine JumpStart and Bedrock in a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8db95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "# Set up SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Deploy a JumpStart model for sentiment analysis\n",
    "sentiment_model = JumpStartModel(model_id=\"huggingface-sentiment-distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "sentiment_predictor = sentiment_model.deploy()\n",
    "\n",
    "# Set up Bedrock client\n",
    "bedrock = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "def analyze_and_respond(user_input):\n",
    "    # Analyze sentiment using JumpStart model\n",
    "    sentiment = sentiment_predictor.predict({\"inputs\": user_input})\n",
    "    sentiment_label = \"positive\" if sentiment[0]['label'] == \"LABEL_1\" else \"negative\"\n",
    "\n",
    "    # Generate response using Bedrock\n",
    "    prompt = f\"\"\"Human: The user's message \"{user_input}\" has been analyzed as having a {sentiment_label} sentiment. \n",
    "    Please generate a response that acknowledges this sentiment and provides a helpful reply.\n",
    "\n",
    "    Assistant: Certainly! Here's a response that acknowledges the sentiment and provides a helpful reply:\n",
    "\n",
    "    {sentiment_label.capitalize()} sentiment detected. I understand that you're feeling {sentiment_label} about this. \n",
    "\n",
    "    Human: Great, now please provide the actual response to the user based on their input and the detected sentiment.\n",
    "\n",
    "    Assistant: Certainly! Here's a response tailored to the user's input and detected sentiment:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    body = json.dumps({\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens_to_sample\": 200,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.8,\n",
    "    })\n",
    "\n",
    "    modelId = 'anthropic.claude-v2'\n",
    "    response = bedrock.invoke_model(body=body, modelId=modelId)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "    return response_body.get('completion').strip()\n",
    "\n",
    "# Example usage\n",
    "user_message = \"I'm really excited about learning AWS services!\"\n",
    "response = analyze_and_respond(user_message)\n",
    "print(f\"User: {user_message}\")\n",
    "print(f\"AI: {response}\")\n",
    "\n",
    "# Clean up\n",
    "sentiment_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d0bfe4",
   "metadata": {},
   "source": [
    "This example demonstrates how you can use a JumpStart model for sentiment analysis and then use that information to generate a more contextually appropriate response with a Bedrock model.\n",
    "\n",
    "By combining these services, you can create sophisticated AI applications that leverage the strengths of both SageMaker JumpStart and Amazon Bedrock."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
