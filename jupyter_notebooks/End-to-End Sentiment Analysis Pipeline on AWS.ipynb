{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c621ae3",
   "metadata": {},
   "source": [
    "# End-to-End Sentiment Analysis Pipeline on AWS\n",
    "\n",
    "This case study demonstrates a complete workflow for sentiment analysis using AWS services. We'll analyze customer reviews to determine sentiment and visualize the results.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- An AWS account with appropriate permissions\n",
    "- AWS CLI configured with your credentials\n",
    "- Python 3.7 or later\n",
    "- Required Python packages: boto3, pandas, numpy\n",
    "\n",
    "Install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3108f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install boto3 pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4d0c67",
   "metadata": {},
   "source": [
    "## Step 1: Data Preparation\n",
    "\n",
    "First, we'll create a sample dataset of customer reviews and upload it to S3. In a real-world scenario, you would replace this with your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115d5387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "# Create sample data\n",
    "np.random.seed(42)\n",
    "n_reviews = 1000\n",
    "\n",
    "products = ['Laptop', 'Smartphone', 'Headphones', 'Smartwatch', 'Tablet']\n",
    "reviews = [\n",
    "    \"This product is amazing! I love it.\",\n",
    "    \"Not bad, but could be better.\",\n",
    "    \"Terrible experience. Would not recommend.\",\n",
    "    \"Great value for money.\",\n",
    "    \"It's okay, nothing special.\"\n",
    "]\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'review_id': range(1, n_reviews + 1),\n",
    "    'product': np.random.choice(products, n_reviews),\n",
    "    'rating': np.random.randint(1, 6, n_reviews),\n",
    "    'review_text': np.random.choice(reviews, n_reviews)\n",
    "})\n",
    "\n",
    "# Upload to S3\n",
    "bucket_name = 'your-bucket-name'  # Replace with your S3 bucket name\n",
    "key = 'reviews_data/customer_reviews.csv'\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "csv_buffer = io.StringIO()\n",
    "data.to_csv(csv_buffer, index=False)\n",
    "s3.put_object(Bucket=bucket_name, Key=key, Body=csv_buffer.getvalue())\n",
    "\n",
    "print(f\"Data uploaded to s3://{bucket_name}/{key}\")\n",
    "\n",
    "# To use your own data, comment out the code above and use:\n",
    "# s3.upload_file('path/to/your/data.csv', bucket_name, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb48f2a",
   "metadata": {},
   "source": [
    "## Step 2: Sentiment Analysis with Amazon Comprehend\n",
    "\n",
    "We'll use Amazon Comprehend to perform sentiment analysis on our customer reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a05b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "comprehend = boto3.client('comprehend')\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    try:\n",
    "        response = comprehend.detect_sentiment(Text=text, LanguageCode='en')\n",
    "        return response['Sentiment'], response['SentimentScore']\n",
    "    except ClientError as e:\n",
    "        print(f\"Error detecting sentiment: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def process_reviews(bucket, key):\n",
    "    # Read the CSV file from S3\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    df = pd.read_csv(io.BytesIO(obj['Body'].read()))\n",
    "    \n",
    "    results = []\n",
    "    for _, row in df.iterrows():\n",
    "        sentiment, scores = analyze_sentiment(row['review_text'])\n",
    "        if sentiment is not None:\n",
    "            results.append({\n",
    "                'review_id': row['review_id'],\n",
    "                'product': row['product'],\n",
    "                'rating': row['rating'],\n",
    "                'sentiment': sentiment,\n",
    "                'positive_score': scores['Positive'],\n",
    "                'negative_score': scores['Negative'],\n",
    "                'neutral_score': scores['Neutral'],\n",
    "                'mixed_score': scores['Mixed']\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Process the reviews\n",
    "results_df = process_reviews(bucket_name, key)\n",
    "\n",
    "# Save results to S3\n",
    "output_key = 'sentiment_results/sentiment_analysis.csv'\n",
    "csv_buffer = io.StringIO()\n",
    "results_df.to_csv(csv_buffer, index=False)\n",
    "s3.put_object(Bucket=bucket_name, Key=output_key, Body=csv_buffer.getvalue())\n",
    "\n",
    "print(f\"Sentiment analysis results saved to s3://{bucket_name}/{output_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e4d11f",
   "metadata": {},
   "source": [
    "## Step 3: Data Analysis and Preparation for Visualization\n",
    "\n",
    "Let's perform some basic analysis on our sentiment results and prepare the data for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b47652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the sentiment analysis results\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=output_key)\n",
    "results_df = pd.read_csv(io.BytesIO(obj['Body'].read()))\n",
    "\n",
    "# Calculate average sentiment scores by product\n",
    "avg_sentiment = results_df.groupby('product').agg({\n",
    "    'positive_score': 'mean',\n",
    "    'negative_score': 'mean',\n",
    "    'neutral_score': 'mean',\n",
    "    'mixed_score': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate sentiment distribution\n",
    "sentiment_dist = results_df['sentiment'].value_counts(normalize=True).reset_index()\n",
    "sentiment_dist.columns = ['sentiment', 'percentage']\n",
    "sentiment_dist['percentage'] = sentiment_dist['percentage'] * 100\n",
    "\n",
    "# Save processed data for QuickSight\n",
    "processed_data_key = 'quicksight_data/processed_sentiment_data.csv'\n",
    "csv_buffer = io.StringIO()\n",
    "results_df.to_csv(csv_buffer, index=False)\n",
    "s3.put_object(Bucket=bucket_name, Key=processed_data_key, Body=csv_buffer.getvalue())\n",
    "\n",
    "print(f\"Processed data for QuickSight saved to s3://{bucket_name}/{processed_data_key}\")\n",
    "\n",
    "# Create a simple visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "sentiment_dist.plot(kind='bar', x='sentiment', y='percentage')\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Percentage')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot to S3\n",
    "img_data = io.BytesIO()\n",
    "plt.savefig(img_data, format='png')\n",
    "img_data.seek(0)\n",
    "s3.put_object(Body=img_data, Bucket=bucket_name, Key='sentiment_analysis/sentiment_distribution.png')\n",
    "\n",
    "print(f\"Sentiment distribution plot saved to s3://{bucket_name}/sentiment_analysis/sentiment_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f22c78",
   "metadata": {},
   "source": [
    "## Step 4: Setting up Amazon QuickSight\n",
    "\n",
    "To visualize the data in QuickSight, follow these steps:\n",
    "\n",
    "1. Sign in to the AWS Management Console and open the Amazon QuickSight console.\n",
    "\n",
    "2. If you haven't used QuickSight before, you'll need to sign up for it and grant it permissions to access your S3 bucket.\n",
    "\n",
    "3. Once in QuickSight, create a new dataset:\n",
    "   - Choose \"New dataset\" and select \"S3\" as the data source.\n",
    "   - Enter a data source name (e.g., \"SentimentAnalysisResults\").\n",
    "   - For the S3 URL, enter: `s3://{bucket_name}/{processed_data_key}` (replace with your actual bucket and key).\n",
    "   - Choose \"Upload\" to upload a manifest file, which QuickSight will create for you.\n",
    "\n",
    "4. After creating the dataset, you can create a new analysis:\n",
    "   - Click \"New analysis\" and select your dataset.\n",
    "   - Use the visual types and fields to create visualizations. For example:\n",
    "     - Create a pie chart of sentiment distribution\n",
    "     - Create a bar chart of average sentiment scores by product\n",
    "     - Create a scatter plot of rating vs. positive sentiment score\n",
    "\n",
    "5. Save your analysis and optionally publish it as a dashboard for easier sharing.\n",
    "\n",
    "## Step 5: Automating the Pipeline\n",
    "\n",
    "To automate this pipeline, you can create an AWS Lambda function that triggers on new data uploads to your S3 bucket. Here's a sample Lambda function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b06aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "comprehend = boto3.client('comprehend')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Get the S3 bucket and key from the event\n",
    "    bucket = event['Records'][0]['s3']['bucket']['name']\n",
    "    key = event['Records'][0]['s3']['object']['key']\n",
    "    \n",
    "    # Process the reviews\n",
    "    results_df = process_reviews(bucket, key)\n",
    "    \n",
    "    # Save results to S3\n",
    "    output_key = f\"sentiment_results/{key.split('/')[-1].replace('.csv', '_analyzed.csv')}\"\n",
    "    csv_buffer = io.StringIO()\n",
    "    results_df.to_csv(csv_buffer, index=False)\n",
    "    s3.put_object(Bucket=bucket, Key=output_key, Body=csv_buffer.getvalue())\n",
    "    \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': f\"Sentiment analysis completed. Results saved to s3://{bucket}/{output_key}\"\n",
    "    }\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    response = comprehend.detect_sentiment(Text=text, LanguageCode='en')\n",
    "    return response['Sentiment'], response['SentimentScore']\n",
    "\n",
    "def process_reviews(bucket, key):\n",
    "    # Read the CSV file from S3\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    df = pd.read_csv(io.BytesIO(obj['Body'].read()))\n",
    "    \n",
    "    results = []\n",
    "    for _, row in df.iterrows():\n",
    "        sentiment, scores = analyze_sentiment(row['review_text'])\n",
    "        results.append({\n",
    "            'review_id': row['review_id'],\n",
    "            'product': row['product'],\n",
    "            'rating': row['rating'],\n",
    "            'sentiment': sentiment,\n",
    "            'positive_score': scores['Positive'],\n",
    "            'negative_score': scores['Negative'],\n",
    "            'neutral_score': scores['Neutral'],\n",
    "            'mixed_score': scores['Mixed']\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a27332b",
   "metadata": {},
   "source": [
    "To set up this Lambda function:\n",
    "\n",
    "1. Create a new Lambda function in the AWS Console.\n",
    "2. Copy the code above into the function.\n",
    "3. Set up an S3 trigger for your input bucket and prefix.\n",
    "4. Make sure the Lambda function has the necessary permissions to access S3 and Comprehend.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This end-to-end example demonstrates how to:\n",
    "\n",
    "1. Prepare and upload customer review data to S3\n",
    "2. Perform sentiment analysis using Amazon Comprehend\n",
    "3. Process and analyze the sentiment results\n",
    "4. Visualize the results using Amazon QuickSight\n",
    "5. Automate the pipeline using AWS Lambda\n",
    "\n",
    "Key points to remember:\n",
    "\n",
    "- Replace the sample data with your own dataset for real-world applications\n",
    "- Ensure proper error handling and logging for production environments\n",
    "- Set up monitoring for the Lambda function to track its performance\n",
    "- Regularly update and retrain your sentiment analysis model if using a custom model\n",
    "- Consider using Amazon Athena for more complex SQL-based analysis of your results\n",
    "\n",
    "By following this workflow, you can build and deploy a scalable sentiment analysis pipeline on AWS, leveraging managed services to minimize operational overhead and focus on deriving insights from your data."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
