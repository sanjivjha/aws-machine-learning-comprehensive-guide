{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "228aa40d",
   "metadata": {},
   "source": [
    "# Chapter 5: Integration and Workflows\n",
    "\n",
    "This chapter focuses on integrating various AWS services to create end-to-end machine learning workflows. We'll cover a complete customer churn prediction example, the use of AWS Step Functions for ML workflows, and best practices for integrating multiple AWS services.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- An AWS account with appropriate permissions\n",
    "- AWS CLI configured with your credentials\n",
    "- Python 3.7 or later\n",
    "- Required Python packages: boto3, pandas, numpy, sagemaker, awswrangler\n",
    "\n",
    "Install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21639911",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install boto3 pandas numpy sagemaker awswrangler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03b86b3",
   "metadata": {},
   "source": [
    "## 1. End-to-End ML Workflow: Customer Churn Prediction\n",
    "\n",
    "We'll create a complete workflow for predicting customer churn, integrating several AWS services.\n",
    "\n",
    "### 1.1 Data Preparation with AWS Glue DataBrew\n",
    "\n",
    "First, we'll use AWS Glue DataBrew to clean and prepare our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca475bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "\n",
    "# Create a sample dataset\n",
    "data = {\n",
    "    'customer_id': range(1, 1001),\n",
    "    'age': np.random.randint(18, 80, 1000),\n",
    "    'tenure': np.random.randint(0, 10, 1000),\n",
    "    'balance': np.random.uniform(0, 250000, 1000),\n",
    "    'num_products': np.random.randint(1, 5, 1000),\n",
    "    'is_active': np.random.choice([0, 1], 1000),\n",
    "    'churn': np.random.choice([0, 1], 1000, p=[0.8, 0.2])  # 20% churn rate\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Upload the dataset to S3\n",
    "bucket_name = 'your-bucket-name'  # Replace with your S3 bucket name\n",
    "wr.s3.to_csv(df, f's3://{bucket_name}/raw_data/customer_data.csv', index=False)\n",
    "\n",
    "# Set up Glue DataBrew client\n",
    "databrew = boto3.client('databrew')\n",
    "\n",
    "# Create DataBrew dataset\n",
    "response = databrew.create_dataset(\n",
    "    Name='customer_churn_dataset',\n",
    "    Format='CSV',\n",
    "    FormatOptions={\n",
    "        'Csv': {\n",
    "            'Delimiter': ',',\n",
    "            'HeaderRow': True\n",
    "        }\n",
    "    },\n",
    "    Input={\n",
    "        'S3InputDefinition': {\n",
    "            'Bucket': bucket_name,\n",
    "            'Key': 'raw_data/customer_data.csv'\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create DataBrew project\n",
    "response = databrew.create_project(\n",
    "    Name='customer_churn_project',\n",
    "    DatasetName='customer_churn_dataset',\n",
    "    RecipeName='customer_churn_recipe'\n",
    ")\n",
    "\n",
    "# Define and create recipe\n",
    "recipe_steps = [\n",
    "    {\n",
    "        'Action': {\n",
    "            'Operation': 'CAST_COLUMN_TYPE',\n",
    "            'Parameters': {\n",
    "                'columnName': 'churn',\n",
    "                'newType': 'INTEGER'\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'Action': {\n",
    "            'Operation': 'NORMALIZE_COLUMN',\n",
    "            'Parameters': {\n",
    "                'columnName': 'balance',\n",
    "                'normalizeType': 'MIN_MAX'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "response = databrew.create_recipe(\n",
    "    Name='customer_churn_recipe',\n",
    "    Steps=recipe_steps\n",
    ")\n",
    "\n",
    "# Create and start job\n",
    "response = databrew.create_job(\n",
    "    Name='customer_churn_job',\n",
    "    ProjectName='customer_churn_project',\n",
    "    RoleArn='your-databrew-role-arn',  # Replace with your DataBrew role ARN\n",
    "    OutputLocation={\n",
    "        'Bucket': bucket_name,\n",
    "        'Key': 'prepared_data/'\n",
    "    }\n",
    ")\n",
    "\n",
    "response = databrew.start_job_run(Name='customer_churn_job')\n",
    "\n",
    "print(\"DataBrew job started. Check AWS console for progress.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4bfd2b",
   "metadata": {},
   "source": [
    "### 1.2 Feature Management with SageMaker Feature Store\n",
    "\n",
    "Next, we'll use SageMaker Feature Store to manage our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fe9d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.feature_store.feature_definition import FeatureDefinition, FeatureTypeEnum\n",
    "\n",
    "# Set up SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "feature_store_session = sagemaker_session\n",
    "\n",
    "# Load prepared data\n",
    "prepared_data = wr.s3.read_csv(f's3://{bucket_name}/prepared_data/')\n",
    "\n",
    "# Define features\n",
    "feature_definitions = [\n",
    "    FeatureDefinition(feature_name=\"customer_id\", feature_type=FeatureTypeEnum.STRING),\n",
    "    FeatureDefinition(feature_name=\"age\", feature_type=FeatureTypeEnum.INTEGRAL),\n",
    "    FeatureDefinition(feature_name=\"tenure\", feature_type=FeatureTypeEnum.INTEGRAL),\n",
    "    FeatureDefinition(feature_name=\"balance\", feature_type=FeatureTypeEnum.FRACTIONAL),\n",
    "    FeatureDefinition(feature_name=\"num_products\", feature_type=FeatureTypeEnum.INTEGRAL),\n",
    "    FeatureDefinition(feature_name=\"is_active\", feature_type=FeatureTypeEnum.INTEGRAL),\n",
    "    FeatureDefinition(feature_name=\"churn\", feature_type=FeatureTypeEnum.INTEGRAL)\n",
    "]\n",
    "\n",
    "# Create Feature Group\n",
    "feature_group = FeatureGroup(\n",
    "    name=\"customer_churn_features\",\n",
    "    feature_definitions=feature_definitions,\n",
    "    sagemaker_session=feature_store_session\n",
    ")\n",
    "\n",
    "# Create the Feature Group\n",
    "feature_group.create(\n",
    "    s3_uri=f\"s3://{bucket_name}/feature_store\",\n",
    "    record_identifier_name=\"customer_id\",\n",
    "    event_time_feature_name=\"event_time\",\n",
    "    role_arn=\"your-sagemaker-role-arn\",  # Replace with your SageMaker role ARN\n",
    "    enable_online_store=True\n",
    ")\n",
    "\n",
    "# Ingest data into the Feature Store\n",
    "import time\n",
    "current_time = int(round(time.time() * 1000))\n",
    "prepared_data['event_time'] = current_time\n",
    "\n",
    "feature_group.ingest(\n",
    "    data_frame=prepared_data,\n",
    "    max_workers=3,\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "print(\"Data ingested into Feature Store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a42b8d4",
   "metadata": {},
   "source": [
    "### 1.3 Model Training with SageMaker\n",
    "\n",
    "Now we'll train a model using SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eef6485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.xgboost import XGBoost\n",
    "\n",
    "# Prepare data for training\n",
    "train_data = sagemaker_session.upload_data(\n",
    "    path=\"train.csv\",\n",
    "    bucket=bucket_name,\n",
    "    key_prefix=\"train\"\n",
    ")\n",
    "\n",
    "# Set up XGBoost estimator\n",
    "xgb = XGBoost(\n",
    "    entry_point=\"train.py\",\n",
    "    framework_version=\"1.0-1\",\n",
    "    hyperparameters={\n",
    "        \"max_depth\": 5,\n",
    "        \"eta\": 0.2,\n",
    "        \"gamma\": 4,\n",
    "        \"min_child_weight\": 6,\n",
    "        \"subsample\": 0.8,\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"num_round\": 100\n",
    "    },\n",
    "    role=\"your-sagemaker-role-arn\",  # Replace with your SageMaker role ARN\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    output_path=f\"s3://{bucket_name}/model\"\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb.fit({\"train\": train_data})\n",
    "\n",
    "print(\"Model training completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba50f41",
   "metadata": {},
   "source": [
    "### 1.4 Model Deployment and Monitoring\n",
    "\n",
    "We'll deploy the trained model and set up monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a119a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig, DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "# Deploy the model\n",
    "predictor = xgb.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    data_capture_config=DataCaptureConfig(\n",
    "        enable_capture=True,\n",
    "        sampling_percentage=100,\n",
    "        destination_s3_uri=f\"s3://{bucket_name}/data-capture\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Set up model monitoring\n",
    "my_monitor = DefaultModelMonitor(\n",
    "    role=\"your-sagemaker-role-arn\",  # Replace with your SageMaker role ARN\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "my_monitor.suggest_baseline(\n",
    "    baseline_dataset=f\"s3://{bucket_name}/train/train.csv\",\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=f\"s3://{bucket_name}/monitoring/baseline\",\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "\n",
    "my_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=\"churn-prediction-monitor\",\n",
    "    endpoint_input=predictor.endpoint_name,\n",
    "    output_s3_uri=f\"s3://{bucket_name}/monitoring/output\",\n",
    "    statistics=my_monitor.baseline_statistics(),\n",
    "    constraints=my_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")\n",
    "\n",
    "print(\"Model deployed and monitoring set up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662625af",
   "metadata": {},
   "source": [
    "### 1.5 Making Predictions\n",
    "\n",
    "Finally, let's use our deployed model to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfc64af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Prepare a sample input\n",
    "sample_input = np.array([[40, 5, 100000, 2, 1]])  # age, tenure, balance, num_products, is_active\n",
    "\n",
    "# Make a prediction\n",
    "result = predictor.predict(sample_input)\n",
    "\n",
    "print(f\"Churn Prediction: {result}\")\n",
    "\n",
    "# Clean up\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cff3bfb",
   "metadata": {},
   "source": [
    "## 2. AWS Step Functions for ML Workflows\n",
    "\n",
    "AWS Step Functions allow you to coordinate multiple AWS services into serverless workflows. Here's an example of how to create a Step Functions workflow for our ML pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e9b4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "stepfunctions = boto3.client('stepfunctions')\n",
    "\n",
    "# Define the state machine\n",
    "definition = {\n",
    "    \"Comment\": \"Customer Churn Prediction Workflow\",\n",
    "    \"StartAt\": \"DataPrep\",\n",
    "    \"States\": {\n",
    "        \"DataPrep\": {\n",
    "            \"Type\": \"Task\",\n",
    "            \"Resource\": \"arn:aws:states:::glue:startJobRun.sync\",\n",
    "            \"Parameters\": {\n",
    "                \"JobName\": \"customer_churn_job\"\n",
    "            },\n",
    "            \"Next\": \"TrainModel\"\n",
    "        },\n",
    "        \"TrainModel\": {\n",
    "            \"Type\": \"Task\",\n",
    "            \"Resource\": \"arn:aws:states:::sagemaker:createTrainingJob.sync\",\n",
    "            \"Parameters\": {\n",
    "                \"TrainingJobName.$\": \"$$.Execution.Name\",\n",
    "                \"AlgorithmSpecification\": {\n",
    "                    \"TrainingImage\": \"...xgboost image arn...\",\n",
    "                    \"TrainingInputMode\": \"File\"\n",
    "                },\n",
    "                \"RoleArn\": \"your-sagemaker-role-arn\",\n",
    "                \"InputDataConfig\": [{\n",
    "                    \"ChannelName\": \"train\",\n",
    "                    \"DataSource\": {\n",
    "                        \"S3DataSource\": {\n",
    "                            \"S3Uri\": f\"s3://{bucket_name}/prepared_data/\",\n",
    "                            \"S3DataType\": \"S3Prefix\"\n",
    "                        }\n",
    "                    }\n",
    "                }],\n",
    "                \"OutputDataConfig\": {\n",
    "                    \"S3OutputPath\": f\"s3://{bucket_name}/model_output/\"\n",
    "                },\n",
    "                \"ResourceConfig\": {\n",
    "                    \"InstanceCount\": 1,\n",
    "                    \"InstanceType\": \"ml.m5.xlarge\",\n",
    "                    \"VolumeSizeInGB\": 30\n",
    "                },\n",
    "                \"HyperParameters\": {\n",
    "                    \"max_depth\": \"5\",\n",
    "                    \"eta\": \"0.2\",\n",
    "                    \"gamma\": \"4\",\n",
    "                    \"min_child_weight\": \"6\",\n",
    "                    \"subsample\": \"0.8\",\n",
    "                    \"objective\": \"binary:logistic\",\n",
    "                    \"num_round\": \"100\"\n",
    "                },\n",
    "                \"StoppingCondition\": {\n",
    "                    \"MaxRuntimeInSeconds\": 86400\n",
    "                }\n",
    "            },\n",
    "            \"Next\": \"DeployModel\"\n",
    "        },\n",
    "        \"DeployModel\": {\n",
    "            \"Type\": \"Task\",\n",
    "            \"Resource\": \"arn:aws:states:::sagemaker:createModel\",\n",
    "            \"Parameters\": {\n",
    "                \"ModelName.$\": \"$$.Execution.Name\",\n",
    "                \"PrimaryContainer\": {\n",
    "                    \"Image\": \"...xgboost image arn...\",\n",
    "                    \"ModelDataUrl.$\": \"$.ModelArtifacts.S3ModelArtifacts\"\n",
    "                },\n",
    "                \"ExecutionRoleArn\": \"your-sagemaker-role-arn\"\n",
    "            },\n",
    "            \"Next\": \"ConfigureEndpoint\"\n",
    "        },\n",
    "        \"ConfigureEndpoint\": {\n",
    "            \"Type\": \"Task\",\n",
    "            \"Resource\": \"arn:aws:states:::sagemaker:createEndpointConfig\",\n",
    "            \"Parameters\": {\n",
    "                \"EndpointConfigName.$\": \"$$.Execution.Name\",\n",
    "                \"ProductionVariants\": [{\n",
    "                    \"InitialInstanceCount\": 1,\n",
    "                    \"InstanceType\": \"ml.m4.xlarge\",\n",
    "                    \"ModelName.$\": \"$$.Execution.Name\",\n",
    "                    \"VariantName\": \"AllTraffic\"\n",
    "                }]\n",
    "            },\n",
    "            \"Next\": \"Deploy\"\n",
    "        },\n",
    "        \"Deploy\": {\n",
    "            \"Type\": \"Task\",\n",
    "            \"Resource\": \"arn:aws:states:::sagemaker:createEndpoint\",\n",
    "            \"Parameters\": {\n",
    "                \"EndpointName.$\": \"$$.Execution.Name\",\n",
    "                \"EndpointConfigName.$\": \"$$.Execution.Name\"\n",
    "            },\n",
    "            \"End\": true\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the state machine\n",
    "response = stepfunctions.create_state_machine(\n",
    "    name='CustomerChurnPredictionWorkflow',\n",
    "    definition=json.dumps(definition),\n",
    "    roleArn='your-step-functions-role-arn'  # Replace with your Step Functions role ARN\n",
    ")\n",
    "\n",
    "print(f\"State Machine ARN: {response['stateMachineArn']}\")\n",
    "\n",
    "# Start execution\n",
    "execution = stepfunctions.start_execution(\n",
    "    stateMachineArn=response['stateMachineArn'],\n",
    "    name='CustomerChurnPrediction-' + str(int(time.time()))\n",
    ")\n",
    "\n",
    "print(f\"Execution ARN: {execution['executionArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13fdcf1",
   "metadata": {},
   "source": [
    "## 3. Integrating AWS Services\n",
    "\n",
    "### 3.1 Combining Multiple Services in a Single Pipeline\n",
    "\n",
    "The examples above demonstrate how to combine multiple AWS services in a single ML pipeline:\n",
    "\n",
    "1. AWS Glue DataBrew for data preparation\n",
    "2. Amazon S3 for data storage\n",
    "3. SageMaker Feature Store for feature management\n",
    "4. SageMaker for model training and deployment\n",
    "5. CloudWatch for monitoring\n",
    "6. Step Functions for workflow orchestration\n",
    "\n",
    "### 3.2 Best Practices for Service Integration\n",
    "\n",
    "When integrating multiple AWS services, consider the following best practices:\n",
    "\n",
    "1. **Use IAM roles**: Create specific IAM roles for each service with the minimum required permissions.\n",
    "\n",
    "2. **Implement error handling**: Use try-except blocks and Step Functions error handling to manage failures gracefully.\n",
    "\n",
    "3. **Use environment variables**: Store configuration details in environment variables or AWS Systems Manager Parameter Store.\n",
    "\n",
    "4. **Implement logging**: Use CloudWatch Logs for centralized logging across all services.\n",
    "\n",
    "5. **Use versioning**: Version your data, models, and code to ensure reproducibility.\n",
    "\n",
    "6. **Implement monitoring**: Set up CloudWatch alarms and dashboards to monitor your entire pipeline.\n",
    "\n",
    "7. **Use infrastructure as code**: Utilize AWS CloudFormation or AWS CDK to define and manage your AWS resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdb6065",
   "metadata": {},
   "outputs": [],
   "source": [
    "Example using AWS CDK:\n",
    "\n",
    "```python\n",
    "from aws_cdk import (\n",
    "    core,\n",
    "    aws_s3 as s3,\n",
    "    aws_sagemaker as sagemaker,\n",
    "    aws_iam as iam,\n",
    ")\n",
    "\n",
    "class MLPipelineStack(core.Stack):\n",
    "    def __init__(self, scope: core.Construct, id: str, **kwargs) -> None:\n",
    "        super().__init__(scope, id, **kwargs)\n",
    "\n",
    "        # Create an S3 bucket\n",
    "        bucket = s3.Bucket(self, \"MLDataBucket\")\n",
    "\n",
    "        # Create a SageMaker execution role\n",
    "        sagemaker_role = iam.Role(self, \"SageMakerExecutionRole\",\n",
    "            assumed_by=iam.ServicePrincipal(\"sagemaker.amazonaws.com\"),\n",
    "            managed_policies=[iam.ManagedPolicy.from_aws_managed_policy_name(\"AmazonSageMakerFullAccess\")]\n",
    "        )\n",
    "\n",
    "        # Create a SageMaker notebook instance\n",
    "        notebook = sagemaker.CfnNotebookInstance(self, \"MLNotebookInstance\",\n",
    "            instance_type=\"ml.t3.medium\",\n",
    "            role_arn=sagemaker_role.role_arn,\n",
    "            root_access=\"Enabled\"\n",
    "        )\n",
    "\n",
    "app = core.App()\n",
    "MLPipelineStack(app, \"MLPipelineStack\")\n",
    "app.synth()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790507e7",
   "metadata": {},
   "source": [
    "8. **Implement data validation**: Validate your data at each step of the pipeline to ensure data quality and consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f683b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Example using Great Expectations:\n",
    "\n",
    "```python\n",
    "import great_expectations as ge\n",
    "\n",
    "# Load your data\n",
    "df = ge.read_csv(\"s3://your-bucket/your-data.csv\")\n",
    "\n",
    "# Create an expectation suite\n",
    "df.expect_column_values_to_be_between(\"age\", min_value=0, max_value=120)\n",
    "df.expect_column_values_to_be_in_set(\"churn\", [0, 1])\n",
    "\n",
    "# Validate the data\n",
    "results = df.validate()\n",
    "\n",
    "if not results[\"success\"]:\n",
    "    raise ValueError(\"Data validation failed\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e39ef5f",
   "metadata": {},
   "source": [
    "9. **Use AWS Step Functions for complex workflows**: For pipelines with multiple steps and decision points, use AWS Step Functions to orchestrate your workflow.\n",
    "\n",
    "10. **Implement CI/CD for ML**: Use services like AWS CodePipeline and AWS CodeBuild to implement continuous integration and deployment for your ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85d24b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Example buildspec.yml for CodeBuild:\n",
    "\n",
    "```yaml\n",
    "version: 0.2\n",
    "\n",
    "phases:\n",
    "  install:\n",
    "    runtime-versions:\n",
    "      python: 3.8\n",
    "    commands:\n",
    "      - pip install pytest sagemaker boto3\n",
    "  build:\n",
    "    commands:\n",
    "      - pytest tests/\n",
    "      - python train.py\n",
    "  post_build:\n",
    "    commands:\n",
    "      - aws sagemaker create-model --model-name $MODEL_NAME --primary-container file://container.json --execution-role-arn $SAGEMAKER_ROLE_ARN\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a115bda",
   "metadata": {},
   "source": [
    "11. **Use SageMaker Pipelines for ML-specific workflows**: For ML-specific workflows, consider using SageMaker Pipelines, which is purpose-built for ML orchestration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a027237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "preprocessing_step = ProcessingStep(\n",
    "    name=\"PreprocessData\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\")],\n",
    "    outputs=[ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "             ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\")],\n",
    "    code=\"preprocess.py\"\n",
    ")\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    name=\"TrainModel\",\n",
    "    estimator=xgb,\n",
    "    inputs={\"train\": TrainingInput(s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "                                    content_type=\"text/csv\")}\n",
    ")\n",
    "\n",
    "model_step = ModelStep(\n",
    "    name=\"CreateModel\",\n",
    "    model=xgb.create_model(),\n",
    "    inputs=training_step.properties.ModelArtifacts.S3ModelArtifacts\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=\"CustomerChurnPipeline\",\n",
    "    steps=[preprocessing_step, training_step, model_step]\n",
    ")\n",
    "\n",
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decd8ab1",
   "metadata": {},
   "source": [
    "12. **Implement proper security measures**: Use encryption at rest and in transit, implement network isolation with VPCs, and use AWS Secrets Manager for managing sensitive information.\n",
    "\n",
    "13. **Optimize for cost**: Use auto-scaling for SageMaker endpoints, leverage Spot Instances where appropriate, and implement lifecycle policies for S3 to manage storage costs.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Integrating multiple AWS services to create end-to-end ML workflows allows you to leverage the best tools for each part of your pipeline while maintaining a cohesive and manageable system. By following best practices and using services like AWS Step Functions or SageMaker Pipelines, you can create robust, scalable, and maintainable ML workflows.\n",
    "\n",
    "Key takeaways from this chapter:\n",
    "\n",
    "1. **End-to-end integration**: We demonstrated how to create a complete ML workflow for customer churn prediction, integrating services like Glue DataBrew, SageMaker Feature Store, SageMaker for training and deployment, and CloudWatch for monitoring.\n",
    "\n",
    "2. **Workflow orchestration**: We explored how to use AWS Step Functions to coordinate complex ML workflows, allowing for better error handling, parallel execution, and visual representation of your pipeline.\n",
    "\n",
    "3. **Best practices**: We covered numerous best practices for integrating AWS services, including using IAM roles, implementing error handling, using infrastructure as code, and implementing CI/CD for ML.\n",
    "\n",
    "4. **Flexibility and scalability**: The approaches discussed in this chapter allow for flexible and scalable ML workflows that can be easily modified and expanded as your needs grow.\n",
    "\n",
    "5. **Managed services**: By leveraging AWS managed services, you can focus on your ML problems rather than infrastructure management.\n",
    "\n",
    "As you build your own ML workflows, remember that the specific services and integration patterns you choose will depend on your unique requirements, data characteristics, and team expertise. Always consider factors such as data volume, model complexity, inference latency requirements, and operational constraints when designing your ML pipelines.\n",
    "\n",
    "In the next chapters, we'll explore more advanced topics such as MLOps practices, handling big data in ML workflows, and implementing advanced monitoring and explainability for your ML models."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
