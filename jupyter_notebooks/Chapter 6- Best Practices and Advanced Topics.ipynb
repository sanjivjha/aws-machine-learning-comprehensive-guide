{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4390bd8",
   "metadata": {},
   "source": [
    "# Chapter 6: Best Practices and Advanced Topics\n",
    "\n",
    "This chapter covers advanced topics and best practices for machine learning on AWS, including MLOps, optimization, security, and scaling.\n",
    "\n",
    "## 1. ML Ops on AWS\n",
    "\n",
    "MLOps (Machine Learning Operations) combines ML systems development and operations to improve the quality and automate the management of ML models in production.\n",
    "\n",
    "### 1.1 Continuous Integration and Deployment for ML\n",
    "\n",
    "Implementing CI/CD for ML involves automating the process of model training, evaluation, and deployment.\n",
    "\n",
    "#### Example using AWS CodePipeline and SageMaker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cdcd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "\n",
    "codepipeline = boto3.client('codepipeline')\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Get the details of the CodePipeline job\n",
    "    job_id = event['CodePipeline.job']['id']\n",
    "    job_data = event['CodePipeline.job']['data']\n",
    "    \n",
    "    try:\n",
    "        # Assume model artifacts are stored in S3\n",
    "        model_data = job_data['inputArtifacts'][0]['location']['s3Location']\n",
    "        \n",
    "        # Create a SageMaker model\n",
    "        model = Model(\n",
    "            model_data=f\"s3://{model_data['bucketName']}/{model_data['objectKey']}\",\n",
    "            role='your-sagemaker-role-arn',\n",
    "            image_uri='your-ecr-image-uri'\n",
    "        )\n",
    "        \n",
    "        # Deploy the model\n",
    "        predictor = model.deploy(\n",
    "            initial_instance_count=1,\n",
    "            instance_type='ml.t2.medium'\n",
    "        )\n",
    "        \n",
    "        # Signal success to CodePipeline\n",
    "        codepipeline.put_job_success_result(jobId=job_id)\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Signal failure to CodePipeline\n",
    "        codepipeline.put_job_failure_result(\n",
    "            jobId=job_id,\n",
    "            failureDetails={'message': str(e), 'type': 'JobFailed'}\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2b3512",
   "metadata": {},
   "source": [
    "### 1.2 Version Control for ML Models\n",
    "\n",
    "Version control for ML models involves tracking changes in data, model architecture, hyperparameters, and performance metrics.\n",
    "\n",
    "#### Example using Amazon S3 versioning and SageMaker model registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec1e1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "sm = boto3.client('sagemaker')\n",
    "\n",
    "# Enable versioning on S3 bucket\n",
    "s3.put_bucket_versioning(\n",
    "    Bucket='your-model-bucket',\n",
    "    VersioningConfiguration={'Status': 'Enabled'}\n",
    ")\n",
    "\n",
    "# Create a model package group\n",
    "sm.create_model_package_group(\n",
    "    ModelPackageGroupName='your-model-group',\n",
    "    ModelPackageGroupDescription='Version controlled model packages'\n",
    ")\n",
    "\n",
    "# After training a model, create a model package\n",
    "model_package = sagemaker.ModelPackage(\n",
    "    role='your-sagemaker-role-arn',\n",
    "    model_package_group_name='your-model-group',\n",
    "    model_data='s3://your-model-bucket/model.tar.gz',\n",
    "    content_types=['text/csv'],\n",
    "    response_types=['text/csv']\n",
    ")\n",
    "\n",
    "model_package.create()\n",
    "\n",
    "# List model package versions\n",
    "response = sm.list_model_packages(\n",
    "    ModelPackageGroupName='your-model-group'\n",
    ")\n",
    "\n",
    "for model_package in response['ModelPackageSummaryList']:\n",
    "    print(f\"Model Package Version: {model_package['ModelPackageVersion']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d705dbc5",
   "metadata": {},
   "source": [
    "## 2. Optimizing ML Workflows\n",
    "\n",
    "Optimization involves improving the efficiency and cost-effectiveness of ML workflows.\n",
    "\n",
    "### 2.1 Cost Optimization Strategies\n",
    "\n",
    "1. Use Spot Instances for training jobs\n",
    "2. Implement autoscaling for inference endpoints\n",
    "3. Use SageMaker Managed Spot Training\n",
    "\n",
    "#### Example of using Spot Instances for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9701d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "estimator = Estimator(\n",
    "    image_uri='your-training-image-uri',\n",
    "    role='your-sagemaker-role-arn',\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.xlarge',\n",
    "    use_spot_instances=True,\n",
    "    max_wait=3600,  # Maximum time to wait for Spot instances (in seconds)\n",
    "    max_run=1800,   # Maximum training time (in seconds)\n",
    ")\n",
    "\n",
    "estimator.fit({'training': 's3://your-bucket/training-data'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfdcefa",
   "metadata": {},
   "source": [
    "### 2.2 Performance Tuning\n",
    "\n",
    "1. Use SageMaker Automatic Model Tuning\n",
    "2. Optimize data input pipeline\n",
    "3. Use appropriate instance types for training and inference\n",
    "\n",
    "#### Example of SageMaker Automatic Model Tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab04bfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner, ContinuousParameter, IntegerParameter\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name='validation:accuracy',\n",
    "    hyperparameter_ranges={\n",
    "        'learning_rate': ContinuousParameter(0.001, 0.1),\n",
    "        'num_layers': IntegerParameter(1, 5),\n",
    "        'num_neurons': IntegerParameter(10, 100)\n",
    "    },\n",
    "    max_jobs=10,\n",
    "    max_parallel_jobs=3\n",
    ")\n",
    "\n",
    "tuner.fit({'training': 's3://your-bucket/training-data', 'validation': 's3://your-bucket/validation-data'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f60a882",
   "metadata": {},
   "source": [
    "## 3. Security in ML Pipelines\n",
    "\n",
    "Ensuring the security of ML pipelines involves protecting data, models, and infrastructure.\n",
    "\n",
    "### 3.1 Encrypting Data at Rest and in Transit\n",
    "\n",
    "1. Use AWS Key Management Service (KMS) for encryption\n",
    "2. Enable encryption for S3 buckets\n",
    "3. Use HTTPS for all API communications\n",
    "\n",
    "#### Example of encrypting S3 data and SageMaker training job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57805a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# Create a KMS key\n",
    "kms = boto3.client('kms')\n",
    "response = kms.create_key(Description='Key for ML data encryption')\n",
    "key_id = response['KeyMetadata']['KeyId']\n",
    "\n",
    "# Enable encryption on S3 bucket\n",
    "s3 = boto3.client('s3')\n",
    "s3.put_bucket_encryption(\n",
    "    Bucket='your-bucket',\n",
    "    ServerSideEncryptionConfiguration={\n",
    "        'Rules': [{'ApplyServerSideEncryptionByDefault': {'SSEAlgorithm': 'aws:kms', 'KMSMasterKeyID': key_id}}]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Use encryption for SageMaker training job\n",
    "estimator = Estimator(\n",
    "    image_uri='your-training-image-uri',\n",
    "    role='your-sagemaker-role-arn',\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.xlarge',\n",
    "    volume_kms_key=key_id,\n",
    "    output_kms_key=key_id,\n",
    "    enable_network_isolation=True\n",
    ")\n",
    "\n",
    "estimator.fit({'training': 's3://your-bucket/training-data'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b4cafa",
   "metadata": {},
   "source": [
    "### 3.2 Managing IAM Roles and Permissions\n",
    "\n",
    "1. Use the principle of least privilege\n",
    "2. Create separate roles for different ML pipeline stages\n",
    "3. Use IAM Access Analyzer to review permissions\n",
    "\n",
    "#### Example of creating a restricted IAM role for SageMaker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75c8396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "iam = boto3.client('iam')\n",
    "\n",
    "assume_role_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"sagemaker.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "role = iam.create_role(\n",
    "    RoleName='RestrictedSageMakerRole',\n",
    "    AssumeRolePolicyDocument=json.dumps(assume_role_policy)\n",
    ")\n",
    "\n",
    "policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:DeleteObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::your-bucket\",\n",
    "                \"arn:aws:s3:::your-bucket/*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"logs:CreateLogGroup\",\n",
    "                \"logs:CreateLogStream\",\n",
    "                \"logs:PutLogEvents\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:logs:*:*:*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "iam.put_role_policy(\n",
    "    RoleName='RestrictedSageMakerRole',\n",
    "    PolicyName='RestrictedSageMakerAccess',\n",
    "    PolicyDocument=json.dumps(policy)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c48cf4",
   "metadata": {},
   "source": [
    "## 4. Scaling ML Workloads\n",
    "\n",
    "Scaling ML workloads involves handling larger datasets, more complex models, and higher inference loads.\n",
    "\n",
    "### 4.1 Using SageMaker Multi-Model Endpoints\n",
    "\n",
    "Multi-model endpoints allow you to deploy multiple models to a single endpoint, sharing compute resources.\n",
    "\n",
    "#### Example of creating a multi-model endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53166ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.multidatamodel import MultiDataModel\n",
    "\n",
    "model_data_prefix = 's3://your-bucket/models'\n",
    "\n",
    "mdm = MultiDataModel(\n",
    "    name='multi-model-endpoint',\n",
    "    model_data_prefix=model_data_prefix,\n",
    "    image_uri='your-inference-image-uri',\n",
    "    role='your-sagemaker-role-arn'\n",
    ")\n",
    "\n",
    "predictor = mdm.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.c5.xlarge'\n",
    ")\n",
    "\n",
    "# Invoke a specific model\n",
    "response = predictor.predict(data, target_model='model1.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb2628f",
   "metadata": {},
   "source": [
    "### 4.2 Distributed Training on SageMaker\n",
    "\n",
    "SageMaker provides built-in support for distributed training using data parallelism and model parallelism.\n",
    "\n",
    "#### Example of data parallel distributed training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5424e433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "distribution = {'dataparallel': {'enabled': True}}\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point='your_training_script.py',\n",
    "    role='your-sagemaker-role-arn',\n",
    "    instance_count=2,\n",
    "    instance_type='ml.p3.16xlarge',\n",
    "    framework_version='1.8.0',\n",
    "    py_version='py3',\n",
    "    distribution=distribution\n",
    ")\n",
    "\n",
    "estimator.fit({'training': 's3://your-bucket/training-data'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668014a5",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Implementing these best practices and advanced topics in your AWS ML workflows will lead to more efficient, secure, and scalable machine learning systems. Key takeaways include:\n",
    "\n",
    "1. Implement MLOps practices for continuous integration, deployment, and version control of ML models.\n",
    "2. Optimize your ML workflows for cost and performance using strategies like Spot Instances and automatic model tuning.\n",
    "3. Ensure the security of your ML pipelines through encryption and proper IAM management.\n",
    "4. Scale your ML workloads using techniques like multi-model endpoints and distributed training.\n",
    "\n",
    "Remember that these practices should be adapted to your specific use case and organizational requirements. Regularly review and update your ML workflows to incorporate new AWS features and industry best practices."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
